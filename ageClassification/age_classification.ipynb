{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated temp directory: /data/mr423/tmp\n",
      "scgpt location:  /data/mr423/project/code/ageClassification/scgpt/__init__.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'scgpt' from '/data/mr423/project/code/ageClassification/scgpt/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 检查 tempfile 模块使用的临时文件目录\n",
    "temp_dir = tempfile.gettempdir()\n",
    "print(\"Updated temp directory:\", temp_dir)\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "new_path = '/data/mr423/project/ageClassification/'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "# relaod the scgpt files\n",
    "import scgpt\n",
    "print(\"scgpt location: \", scgpt.__file__)\n",
    "importlib.reload(scgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "\n",
    "import argparse\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"WANDB_MODE\"]= \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'do_train': True, 'load_model': '/data/mr423/project/pre_trained_model/scGPT_human', 'n_bins': 101, 'epochs': 1, 'lr': 0.001, 'batch_size': 32, 'layer_size': 128, 'nlayers': 4, 'nhead': 8, 'dropout': 0.0, 'use_fast_transformer': True, 'pre_norm': False, 'amp': True, 'freeze': True}\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Settings for wandb mentior\n",
    "######################################################################\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    do_train=True,\n",
    "    load_model=\"/data/mr423/project/pre_trained_model/scGPT_human\",\n",
    "    n_bins=101,\n",
    "\n",
    "    epochs=1, # 50 !!!!!!!!!!!!  test only\n",
    "    lr=0.001,\n",
    "    batch_size=32,   # 128 !!!!!!!!!!!!  test only\n",
    "\n",
    "    layer_size=128, # 128\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=8,  # number of heads in nn.MultiheadAttention\n",
    "    \n",
    "    dropout=0.0,  # dropout probability\n",
    "\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    freeze = True, #freeze\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"age_clusters-test\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for input and preprocessing\n",
    "######################################################################\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "input_emb_style = \"category\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "\n",
    "\n",
    "# settings for training\n",
    "CLS = True  # celltype classification objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for optimizer\n",
    "######################################################################\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "early_stop = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for the model\n",
    "######################################################################\n",
    "use_fast_transformer = config.use_fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "\n",
    "\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Validate the settings\n",
    "######################################################################\n",
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "# assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to ../save/dev_biobank-age-cluster-Aug28-14-25\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Settings for the running recording\n",
    "######################################################################\n",
    "dataset_name = 'biobank-age-cluster'\n",
    "save_dir = Path(f\"../save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37304, 2919)\n",
      "(4145, 2919)\n",
      "{0: '30-40', 1: '40-50', 2: '50-60', 3: '60-70', 4: '70-80'}\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Data loading\n",
    "######################################################################\n",
    "adata = sc.read(\"/data/mr423/project/data/3-OLINK_data_train_withOutlier_all.h5ad\")\n",
    "adata_test = sc.read(\"/data/mr423/project/data/3-OLINK_data_test_withOutlier_all.h5ad\")\n",
    "\n",
    "print(adata.shape)\n",
    "print(adata_test.shape)\n",
    "\n",
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\" \n",
    "\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "\n",
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "adata_test_raw = adata_test.copy()\n",
    "adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "\n",
    "ageGroup_id_labels = adata.obs[\"Age_Group\"].astype(\"category\").cat.codes.values\n",
    "\n",
    "# ageGroup_types = adata.obs[\"Age_Group\"].unique()\n",
    "\n",
    "num_types = len(np.unique(ageGroup_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"Age_Group\"].astype(\"category\").cat.categories))\n",
    "print(id2type)\n",
    "\n",
    "adata.obs[\"ageGroup_id\"] = ageGroup_id_labels\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>DoB_Year</th>\n",
       "      <th>DoB_Month</th>\n",
       "      <th>DoB_Day</th>\n",
       "      <th>DoB</th>\n",
       "      <th>Date_Attend</th>\n",
       "      <th>age</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>str_batch</th>\n",
       "      <th>ageGroup_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2144829-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1939</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1939-01-15</td>\n",
       "      <td>2007-11-16</td>\n",
       "      <td>68.835044</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154285-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-01-15</td>\n",
       "      <td>2007-07-20</td>\n",
       "      <td>62.507871</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679423-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1945</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-11-15</td>\n",
       "      <td>2009-05-19</td>\n",
       "      <td>63.507187</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172610-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1941-12-15</td>\n",
       "      <td>2009-09-23</td>\n",
       "      <td>67.772758</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011532-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1954</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1954-01-15</td>\n",
       "      <td>2009-10-20</td>\n",
       "      <td>55.761807</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503594-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1947</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1947-02-15</td>\n",
       "      <td>2009-08-13</td>\n",
       "      <td>62.491444</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494250-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1945</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-12-15</td>\n",
       "      <td>2009-11-10</td>\n",
       "      <td>63.904175</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746191-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1951</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1951-12-15</td>\n",
       "      <td>2009-06-26</td>\n",
       "      <td>57.530459</td>\n",
       "      <td>50-60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342815-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1942</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1942-08-15</td>\n",
       "      <td>2008-03-11</td>\n",
       "      <td>65.571526</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6020978-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1947</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1947-12-15</td>\n",
       "      <td>2009-07-13</td>\n",
       "      <td>61.577002</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41449 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex  DoB_Year  DoB_Month  DoB_Day         DoB Date_Attend  \\\n",
       "Id                                                                     \n",
       "2144829-0    0      1939          1       15  1939-01-15  2007-11-16   \n",
       "3154285-0    0      1945          1       15  1945-01-15  2007-07-20   \n",
       "1679423-0    1      1945         11       15  1945-11-15  2009-05-19   \n",
       "1172610-0    1      1941         12       15  1941-12-15  2009-09-23   \n",
       "4011532-0    1      1954          1       15  1954-01-15  2009-10-20   \n",
       "...        ...       ...        ...      ...         ...         ...   \n",
       "2503594-1    0      1947          2       15  1947-02-15  2009-08-13   \n",
       "3494250-1    1      1945         12       15  1945-12-15  2009-11-10   \n",
       "5746191-1    1      1951         12       15  1951-12-15  2009-06-26   \n",
       "4342815-1    0      1942          8       15  1942-08-15  2008-03-11   \n",
       "6020978-1    1      1947         12       15  1947-12-15  2009-07-13   \n",
       "\n",
       "                 age Age_Group  batch_id str_batch  ageGroup_id  \n",
       "Id                                                               \n",
       "2144829-0  68.835044     60-70         0         0            3  \n",
       "3154285-0  62.507871     60-70         0         0            3  \n",
       "1679423-0  63.507187     60-70         0         0            3  \n",
       "1172610-0  67.772758     60-70         0         0            3  \n",
       "4011532-0  55.761807     50-60         0         0            2  \n",
       "...              ...       ...       ...       ...          ...  \n",
       "2503594-1  62.491444     60-70         1         1            3  \n",
       "3494250-1  63.904175     60-70         1         1            3  \n",
       "5746191-1  57.530459     50-60         1         1            2  \n",
       "4342815-1  65.571526     60-70         1         1            3  \n",
       "6020978-1  61.577002     60-70         1         1            3  \n",
       "\n",
       "[41449 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2895/2919 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /data/mr423/project/pre_trained_model/scGPT_human/best_model.pt, the model args will override the config /data/mr423/project/pre_trained_model/scGPT_human/args.json.\n",
      "\n",
      "**** parameters from the pre-trained model ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** parameters from the pre-trained model ****\n",
      "\n",
      "**** actual model parameters ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** actual model parameters ****\n",
      "\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The pre-trained model\n",
    "######################################################################\n",
    "if config.load_model is not None:\n",
    "    model_dir = config.load_model\n",
    "    model_config_file = model_dir + \"/args.json\"\n",
    "    model_file = model_dir + \"/best_model.pt\"\n",
    "    vocab_file = model_dir + \"/vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "    print(\"\\n**** parameters from the pre-trained model ****\")\n",
    "    print(f'layer_size = embsize: {model_configs[\"embsize\"]} = d_hid: {model_configs[\"d_hid\"]}, n_layers: {model_configs[\"nlayers\"]}, nhead: {model_configs[\"nheads\"]}')\n",
    "    print(\"**** parameters from the pre-trained model ****\\n\")\n",
    "\n",
    "    print(\"**** actual model parameters ****\")\n",
    "    print(f'layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "    print(\"**** actual model parameters ****\\n\")\n",
    "\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=3000,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Split the data to train and test\n",
    "######################################################################\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "ageGroup_labels = adata.obs[\"ageGroup_id\"].tolist()  # make sure count from 0\n",
    "ageGroup_labels = np.array(ageGroup_labels)\n",
    "\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_ageGroup,\n",
    "    valid_ageGroup,\n",
    ") = train_test_split(\n",
    "    all_counts, ageGroup_labels, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 29843, \n",
      "\t feature length: 2896\n",
      "scGPT - INFO - valid set number of samples: 7461, \n",
      "\t feature length: 2896\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Tokenize the data\n",
    "######################################################################\n",
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "\n",
    "    input_values_train, input_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_ageGroup_train = torch.from_numpy(train_ageGroup).long()\n",
    "    tensor_ageGroup_valid = torch.from_numpy(valid_ageGroup).long()\n",
    "\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"ageGroup\": tensor_ageGroup_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"ageGroup\": tensor_ageGroup_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[60695,  8381,  8385,  ..., 12884, 35421, 60694],\n",
      "        [60695,  8381,  8385,  ...,  4939, 12884, 35421],\n",
      "        [60695,  8381,  8385,  ..., 60694, 60694, 60694],\n",
      "        ...,\n",
      "        [60695,  8385,  8390,  ..., 60694, 60694, 60694],\n",
      "        [60695,  8381,  8385,  ..., 35421, 60694, 60694],\n",
      "        [60695,  8381,  8385,  ..., 60694, 60694, 60694]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[\"genes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,  63.,  70.,  ...,  39.,  83., 101.],\n",
      "        [  0.,  78.,  82.,  ...,  36.,   1.,  66.],\n",
      "        [  0.,  70.,   9.,  ..., 101., 101., 101.],\n",
      "        ...,\n",
      "        [  0.,  79.,  92.,  ..., 101., 101., 101.],\n",
      "        [  0.,  36.,  84.,  ...,  43., 101., 101.],\n",
      "        [  0.,  48.,  63.,  ..., 101., 101., 101.]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[\"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: encoder.embedding.weight\n",
      "freezing weights for: encoder.embedding.weight\n",
      "name: encoder.enc_norm.weight\n",
      "freezing weights for: encoder.enc_norm.weight\n",
      "name: encoder.enc_norm.bias\n",
      "freezing weights for: encoder.enc_norm.bias\n",
      "name: value_encoder.embedding.weight\n",
      "freezing weights for: value_encoder.embedding.weight\n",
      "name: value_encoder.enc_norm.weight\n",
      "freezing weights for: value_encoder.enc_norm.weight\n",
      "name: value_encoder.enc_norm.bias\n",
      "freezing weights for: value_encoder.enc_norm.bias\n",
      "name: transformer_encoder.layers.0.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.0.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.weight\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.bias\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.weight\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.bias\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.weight\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.bias\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.weight\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.bias\n",
      "name: transformer_encoder.layers.1.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.1.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.weight\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.bias\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.weight\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.bias\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.weight\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.bias\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.weight\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.bias\n",
      "name: transformer_encoder.layers.2.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.2.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.weight\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.bias\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.weight\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.bias\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.weight\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.bias\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.weight\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.bias\n",
      "name: transformer_encoder.layers.3.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.3.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.weight\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.bias\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.weight\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.bias\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.weight\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.bias\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.weight\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.bias\n",
      "name: transformer_encoder.layers.4.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.4.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.weight\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.bias\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.weight\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.bias\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.weight\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.bias\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.weight\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.bias\n",
      "name: transformer_encoder.layers.5.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.5.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.weight\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.bias\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.weight\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.bias\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.weight\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.bias\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.weight\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.bias\n",
      "name: transformer_encoder.layers.6.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.6.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.weight\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.bias\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.weight\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.bias\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.weight\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.bias\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.weight\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.bias\n",
      "name: transformer_encoder.layers.7.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.7.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.weight\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.bias\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.weight\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.bias\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.weight\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.bias\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.weight\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.bias\n",
      "name: transformer_encoder.layers.8.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.8.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.weight\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.bias\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.weight\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.bias\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.weight\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.bias\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.weight\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.bias\n",
      "name: transformer_encoder.layers.9.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.9.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.weight\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.bias\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.weight\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.bias\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.weight\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.bias\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.weight\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.bias\n",
      "name: transformer_encoder.layers.10.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.10.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.weight\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.bias\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.weight\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.bias\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.weight\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.bias\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.weight\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.bias\n",
      "name: transformer_encoder.layers.11.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.11.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.weight\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.bias\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.weight\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.bias\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.weight\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.bias\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.weight\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.bias\n",
      "name: cls_decoder._decoder.0.weight\n",
      "name: cls_decoder._decoder.0.bias\n",
      "name: cls_decoder._decoder.2.weight\n",
      "name: cls_decoder._decoder.2.bias\n",
      "name: cls_decoder._decoder.3.weight\n",
      "name: cls_decoder._decoder.3.bias\n",
      "name: cls_decoder._decoder.5.weight\n",
      "name: cls_decoder._decoder.5.bias\n",
      "name: cls_decoder.out_layer.weight\n",
      "name: cls_decoder.out_layer.bias\n",
      "name: linear.weight\n",
      "name: linear.bias\n",
      "name: reg_decoder.fc1.weight\n",
      "name: reg_decoder.fc1.bias\n",
      "name: reg_decoder.bn1.weight\n",
      "name: reg_decoder.bn1.bias\n",
      "name: reg_decoder.fc2.weight\n",
      "name: reg_decoder.fc2.bias\n",
      "name: reg_decoder.bn2.weight\n",
      "name: reg_decoder.bn2.bias\n",
      "name: reg_decoder.fc4.weight\n",
      "name: reg_decoder.fc4.bias\n",
      "scGPT - INFO - Total Pre freeze Params 51255814\n",
      "scGPT - INFO - Total Post freeze Params 1188358\n",
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): CategoryValueEncoder(\n",
      "    (embedding): Embedding(103, 512, padding_idx=101)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls_decoder): ClsDecoder(\n",
      "    (_decoder): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_layer): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (reg_decoder): RegressionEncoder(\n",
      "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation1): LeakyReLU(negative_slope=0.01)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation2): LeakyReLU(negative_slope=0.01)\n",
      "    (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load the model\n",
    "######################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=False,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    num_batch_labels=False,\n",
    "    domain_spec_batchnorm=False,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    explicit_zero_prob=False,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        # for k, v in pretrained_dict.items():\n",
    "        #     logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    # print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    # if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(model)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Loss function\n",
    "######################################################################\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma=0.9)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Train the model\n",
    "######################################################################\n",
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_num = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        ageGroup = batch_data[\"ageGroup\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=CLS,\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=False,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            loss = 0.0\n",
    "            loss = criterion_cls(output_dict[\"cls_output\"], ageGroup)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # with warnings.catch_warnings(record=True) as w:\n",
    "        #     warnings.filterwarnings(\"always\")\n",
    "        #     torch.nn.utils.clip_grad_norm_(\n",
    "        #         model.parameters(),\n",
    "        #         1.0,\n",
    "        #         error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "        #     )\n",
    "        #     if len(w) > 0:\n",
    "        #         logger.warning(\n",
    "        #             f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "        #             f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "        #             \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "        #         )\n",
    "\n",
    "        total_loss += loss.item() * len(input_gene_ids)\n",
    "        total_num += len(input_gene_ids)\n",
    "\n",
    "        # 记录预测值和标签，用于计算二分类指标\n",
    "        preds = output_dict[\"cls_output\"].argmax(dim=1).cpu().numpy()\n",
    "        labels = ageGroup.cpu().numpy()\n",
    "\n",
    "        # 将所有批次的预测值和真实值连接起来\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # 计算二分类指标\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "    # 定义评估指标函数\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    # conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    epoch_loss = total_loss / total_num\n",
    "    epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "    \n",
    "    logger.info(\n",
    "    f\"| train | total_num {total_num} | epoch loss {epoch_loss:5.4f} | precision {precision:5.4f} | \" \n",
    "    f\" recall {recall:5.4f} | f1 {f1:5.4f} | \" \n",
    "    f\" epoch_accuracy {epoch_accuracy:5.4f}\")\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "\n",
    "            \"train/precision\": precision,\n",
    "            \"train/recall\": recall,\n",
    "            \"train/f1\": f1,\n",
    "            \"train/loss\": epoch_loss,\n",
    "            \"train/epoch_accuracy\": epoch_accuracy,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Evaluate the model\n",
    "######################################################################\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    total_truth = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            ageGroup = batch_data[\"ageGroup\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=False,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "\n",
    "                loss = criterion_cls(output_dict[\"cls_output\"], ageGroup)\n",
    "            \n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "            \n",
    "            # 记录预测值和标签，用于计算二分类指标\n",
    "            preds = output_dict[\"cls_output\"].argmax(dim=1).cpu().numpy()\n",
    "            labels = ageGroup.cpu().numpy()\n",
    "\n",
    "            # 将所有批次的预测值和真实值连接起来\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # 计算二分类指标\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "    # 定义评估指标函数\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    # conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    epoch_loss = total_loss / total_num\n",
    "    epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "     \n",
    "    wandb.log(\n",
    "        {\n",
    "\n",
    "            \"valid/precision\": precision,\n",
    "            \"valid/recall\": recall,\n",
    "            \"valid/f1\": f1,\n",
    "            \"valid/loss\": epoch_loss,\n",
    "            \"valid/epoch_accuracy\": epoch_accuracy,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    logger.info(\n",
    "    f\"| valid | total_num {total_num} | epoch loss {epoch_loss:5.4f} | precision {precision:5.4f} | \" \n",
    "    f\" recall {recall:5.4f} | f1 {f1:5.4f} | \" \n",
    "    f\" epoch_accuracy {epoch_accuracy:5.4f}\")\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "\n",
    "    return total_loss / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Apply the model on the age of the clusters \n",
      "\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Epoch 1/1\n",
      "scGPT - INFO - | train | total_num 29843 | epoch loss 1.1127 | precision 0.3495 |  recall 0.4296 | f1 0.3171 |  epoch_accuracy 0.4296\n",
      "scGPT - INFO - | valid | total_num 7461 | epoch loss 1.1026 | precision 0.1923 |  recall 0.4385 | f1 0.2674 |  epoch_accuracy 0.4385\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.1026\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# 训练和验证循环\n",
    "######################################################################\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "\n",
    "logger.info(\"Apply the model on the age of the clusters \\n\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    train_data_pt, valid_data_pt = prepare_data()\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(model,loader=train_loader,)\n",
    "   \n",
    "    val_loss = evaluate(model,loader=valid_loader)\n",
    "\n",
    "    # early stop\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# save the model into the save_dir\n",
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | valid | total_num 4145 | weighted precision 0.2009 |  weighted recall 0.4483 | weighted f1 0.2775 |  weighted accuracy 0.4483\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "precision: [0.        0.        0.4482509 0.       ]\n",
      "recall: [0. 0. 1. 0.]\n",
      "f1: [0.         0.         0.61902382 0.        ]\n",
      "accuracy: 0.45\n",
      "conf_matrix: [[   0    0  942    0]\n",
      " [   0    0 1329    0]\n",
      " [   0    0 1858    0]\n",
      " [   0    0   16    0]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b88627bcbcc4ef19f6fbfa779680498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>info/post_freeze_param_count</td><td>▁</td></tr><tr><td>info/pre_freeze_param_count</td><td>▁</td></tr><tr><td>train/epoch_accuracy</td><td>▁</td></tr><tr><td>train/f1</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/precision</td><td>▁</td></tr><tr><td>train/recall</td><td>▁</td></tr><tr><td>valid/epoch_accuracy</td><td>▁</td></tr><tr><td>valid/f1</td><td>▁</td></tr><tr><td>valid/loss</td><td>▁</td></tr><tr><td>valid/precision</td><td>▁</td></tr><tr><td>valid/recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>info/post_freeze_param_count</td><td>1188358</td></tr><tr><td>info/pre_freeze_param_count</td><td>51255814</td></tr><tr><td>train/epoch_accuracy</td><td>0.42958</td></tr><tr><td>train/f1</td><td>0.3171</td></tr><tr><td>train/loss</td><td>1.11275</td></tr><tr><td>train/precision</td><td>0.34954</td></tr><tr><td>train/recall</td><td>0.42958</td></tr><tr><td>valid/epoch_accuracy</td><td>0.43855</td></tr><tr><td>valid/f1</td><td>0.26739</td></tr><tr><td>valid/loss</td><td>1.1026</td></tr><tr><td>valid/precision</td><td>0.19232</td></tr><tr><td>valid/recall</td><td>0.43855</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /data/mr423/project/code/ageClassification/wandb/offline-run-20240824_090338-81741g0k<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20240824_090338-81741g0k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "######################################################################\n",
    "# Test the model\n",
    "######################################################################\n",
    "def test(model: nn.Module, adata: DataLoader):\n",
    "    \n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    # print(adata.layers[input_layer_key])\n",
    "\n",
    "    ageGroup_labels = adata.obs[\"ageGroup_id\"].tolist()\n",
    "    ageGroup_labels = np.array(ageGroup_labels)\n",
    "\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=False,\n",
    "    )\n",
    "\n",
    "    tensor_age_test = torch.from_numpy(ageGroup_labels).long()\n",
    "\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": tokenized_test[\"values\"],\n",
    "        \"ageGroup\": tensor_age_test,\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            ageGroup = batch_data[\"ageGroup\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=None,\n",
    "                    CLS=True, \n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=False,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                    \n",
    "                loss = criterion_cls(output_dict[\"cls_output\"], ageGroup)\n",
    "            \n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "                   \n",
    "            # 记录预测值和标签，用于计算二分类指标\n",
    "            preds = output_dict[\"cls_output\"].argmax(dim=1).cpu().numpy()\n",
    "            targets = ageGroup.cpu().numpy()\n",
    "\n",
    "            # 将所有批次的预测值和真实值连接起来\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "        # 计算二分类指标\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_targets = np.array(all_targets)\n",
    "    \n",
    "    \n",
    "    # 将 Tensor 转换为 Python 列表\n",
    "    all_preds_list = all_preds.tolist()\n",
    "    all_targets_list = all_targets.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 定义评估指标函数\n",
    "    precision = precision_score(all_targets_list, all_preds_list, average='weighted')\n",
    "    recall = recall_score(all_targets_list, all_preds_list, average='weighted')\n",
    "    f1 = f1_score(all_targets_list, all_preds_list, average='weighted')\n",
    "    accuracy = accuracy_score(all_targets_list, all_preds_list)\n",
    "\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\"-\" * 89)\n",
    "    \n",
    "    logger.info(\n",
    "    f\"| valid | total_num {total_num} | weighted precision {precision:5.4f} | \" \n",
    "    f\" weighted recall {recall:5.4f} | weighted f1 {f1:5.4f} | \" \n",
    "    f\" weighted accuracy {accuracy:5.4f}\")\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    # save to dataFrame\n",
    "    all_preds_save_dir = str(save_dir) + \"/all_preds.txt\"\n",
    "    with open(all_preds_save_dir, \"w\") as file:\n",
    "        file.write(str(all_preds_list))\n",
    "    \n",
    "    all_targets_save_dir = str(save_dir) + \"/all_targets.txt\"\n",
    "    with open(all_targets_save_dir, \"w\") as file:\n",
    "        file.write(str(all_targets_list))\n",
    "\n",
    "    # 创建 DataFrame\n",
    "    all_preds_df = pd.DataFrame(all_preds_list, columns=['Predictions'])\n",
    "    all_targets_df = pd.DataFrame(all_targets_list, columns=['Target'])\n",
    "\n",
    "    # 合并 DataFrame\n",
    "    combined_df = pd.concat([all_preds_df, all_targets_df], axis=1)\n",
    "\n",
    "    combined_df['Predictions'] = combined_df['Predictions'].round(1)\n",
    "    combined_df['Target'] = combined_df['Target'].round(1)\n",
    "\n",
    "    # 文件路径\n",
    "    data_save_dir = str(save_dir) + \"/output.csv\"\n",
    "\n",
    "    # 保存数据到 CSV 文件\n",
    "    combined_df.to_csv(data_save_dir, index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "test(model, adata_test)\n",
    "\n",
    "\n",
    "df = pd.read_csv(str(save_dir) + \"/output.csv\")\n",
    "\n",
    "# 提取预测值和实际值\n",
    "y_pred = df['Predictions']\n",
    "y_true = df['Target']\n",
    "\n",
    "\n",
    "\n",
    "# 定义评估指标函数\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "f1 = f1_score(y_true, y_pred, average=None)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1: {f1}')\n",
    "print(f'accuracy: {accuracy:.2f}')\n",
    "print(f'conf_matrix: {conf_matrix}')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
