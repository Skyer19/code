{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated temp directory: /data/mr423/tmp\n",
      "scgpt location:  /data/mr423/project/code/ageClassification/scgpt/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'do_train': True, 'load_model': '/data/mr423/project/pre_trained_model/scGPT_human', 'n_bins': 101, 'epochs': 15, 'lr': 0.0001, 'batch_size': 128, 'layer_size': 512, 'nlayers': 12, 'nhead': 8, 'dropout': 0.0, 'use_fast_transformer': True, 'pre_norm': False, 'amp': True, 'freeze': True}\n",
      "save to /data/mr423/project/code/ageClassification/record/dev_biobank-age-cluster-Sep07-22-36\n",
      "(37304, 2919)\n",
      "(4145, 2919)\n",
      "{0: '30-40', 1: '40-50', 2: '50-60', 3: '60-70', 4: '70-80'}\n",
      "scGPT - INFO - match 2895/2919 genes in vocabulary of size 60697.\n",
      "**** actual model parameters ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** actual model parameters ****\n",
      "\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 检查 tempfile 模块使用的临时文件目录\n",
    "temp_dir = tempfile.gettempdir()\n",
    "print(\"Updated temp directory:\", temp_dir)\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "new_path = '/data/mr423/project/ageClassification/'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "# relaod the scgpt files\n",
    "import scgpt\n",
    "print(\"scgpt location: \", scgpt.__file__)\n",
    "importlib.reload(scgpt)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"WANDB_MODE\"]= \"offline\"\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for wandb mentior\n",
    "######################################################################\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    do_train=True,\n",
    "    load_model=\"/data/mr423/project/pre_trained_model/scGPT_human\",\n",
    "    n_bins=101,\n",
    "\n",
    "    epochs=15, # 2 !!!!!!!!!!!!  test only\n",
    "    lr=0.0001,\n",
    "    batch_size=128,   # 128 !!!!!!!!!!!!  test only\n",
    "\n",
    "    layer_size=512, # 128\n",
    "    nlayers=12,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=8,  # number of heads in nn.MultiheadAttention\n",
    "    \n",
    "    dropout=0.0,  # dropout probability\n",
    "\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    freeze = True, #freeze\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"age_clusters\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for input and preprocessing\n",
    "######################################################################\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "input_emb_style = \"category\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "\n",
    "\n",
    "# settings for training\n",
    "CLS = True  # celltype classification objective\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for optimizer\n",
    "######################################################################\n",
    "lr = config.lr\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "early_stop = 10\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for the model\n",
    "######################################################################\n",
    "use_fast_transformer = config.use_fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "\n",
    "\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Validate the settings\n",
    "######################################################################\n",
    "# assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "# assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for the running recording\n",
    "######################################################################\n",
    "dataset_name = 'biobank-age-cluster'\n",
    "save_dir = Path(f\"/data/mr423/project/code/ageClassification/record/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Data loading\n",
    "######################################################################\n",
    "adata = sc.read(\"/data/mr423/project/data/3-OLINK_data_train_withOutlier_all.h5ad\")\n",
    "adata_test = sc.read(\"/data/mr423/project/data/3-OLINK_data_test_withOutlier_all.h5ad\")\n",
    "\n",
    "print(adata.shape)\n",
    "print(adata_test.shape)\n",
    "\n",
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\" \n",
    "\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "\n",
    "data_is_raw = False\n",
    "\n",
    "adata_test_raw = adata_test.copy()\n",
    "adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "\n",
    "ageGroup_id_labels = adata.obs[\"Age_Group\"].astype(\"category\").cat.codes.values\n",
    "# ageGroup_types = adata.obs[\"Age_Group\"].unique()\n",
    "adata.obs[\"ageGroup_id\"] = ageGroup_id_labels\n",
    "\n",
    "\n",
    "n_cls = len(np.unique(ageGroup_id_labels))\n",
    "\n",
    "id2type = dict(enumerate(adata.obs[\"Age_Group\"].astype(\"category\").cat.categories))\n",
    "print(id2type)\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The pre-trained model\n",
    "######################################################################\n",
    "if config.load_model is not None:\n",
    "    model_dir = config.load_model\n",
    "    # model_config_file = model_dir + \"/args.json\"\n",
    "\n",
    "    model_file = \"./record/dev_biobank-age-cluster-Sep05-21-37/model.pt\"\n",
    "    vocab_file = \"./record/dev_biobank-age-cluster-Sep05-21-37/vocab.json\"\n",
    "\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "\n",
    "    print(\"**** actual model parameters ****\")\n",
    "    print(f'layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "    print(\"**** actual model parameters ****\\n\")\n",
    "\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=False,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=3000,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)\n",
    "\n",
    "######################################################################\n",
    "# Split the data to train and test\n",
    "######################################################################\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "ageGroup_labels = adata.obs[\"ageGroup_id\"].tolist()  # make sure count from 0\n",
    "ageGroup_labels = np.array(ageGroup_labels)\n",
    "\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_ageGroup,\n",
    "    valid_ageGroup,\n",
    ") = train_test_split(\n",
    "    all_counts, ageGroup_labels, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " **** load model parameters ****\n",
      "lr = 0.0001, batch_size = 128, epochs = 15\n",
      "ntokens = 60697, layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** load model parameters ****\n",
      "\n",
      "scGPT - INFO - Loading SOME model params from ./record/dev_biobank-age-cluster-Sep05-21-37/model.pt\n",
      "--------------------\n",
      "--------------------\n",
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): CategoryValueEncoder(\n",
      "    (embedding): Embedding(103, 512, padding_idx=101)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (classified_decoder): ClassificationDecoder(\n",
      "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation1): LeakyReLU(negative_slope=0.01)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation2): LeakyReLU(negative_slope=0.01)\n",
      "    (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation3): LeakyReLU(negative_slope=0.01)\n",
      "    (fc4): Linear(in_features=128, out_features=5, bias=True)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load the model\n",
    "######################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "\n",
    "print(\"\\n\\n **** load model parameters ****\")\n",
    "print(f'lr = {lr}, batch_size = {batch_size}, epochs = {epochs}')\n",
    "print(f'ntokens = {ntokens}, layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "print(\"**** load model parameters ****\\n\")\n",
    "\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=n_cls,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=False,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    num_batch_labels=False,\n",
    "    domain_spec_batchnorm=False,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    explicit_zero_prob=False,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading ALL model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        logger.info(f\"Loading SOME model params from {model_file}\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        # for k, v in pretrained_dict.items():\n",
    "        #     logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"-\"*20)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "print(model)\n",
    "wandb.watch(model)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Loss function\n",
    "######################################################################\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = (\n",
    "    adata_test.layers[input_layer_key].A\n",
    "    if issparse(adata_test.layers[input_layer_key])\n",
    "    else adata_test.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "# print(adata.layers[input_layer_key])\n",
    "\n",
    "ageGroup_labels = adata_test.obs[\"ageGroup_id\"].tolist()\n",
    "ageGroup_labels = np.array(ageGroup_labels)\n",
    "\n",
    "\n",
    "tokenized_test = tokenize_and_pad_batch(\n",
    "    all_counts,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "\n",
    "tensor_age_test = torch.from_numpy(ageGroup_labels).long()\n",
    "\n",
    "\n",
    "test_data_pt = {\n",
    "    \"gene_ids\": tokenized_test[\"genes\"],\n",
    "    \"values\": tokenized_test[\"values\"],\n",
    "    \"ageGroup\": tensor_age_test,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=SeqDataset(test_data_pt),\n",
    "    batch_size=eval_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_grad(model, input_gene_ids, input_values, ageGroup, vocab, pad_token, noise_level=0.1, num_samples=50):\n",
    "\n",
    "    model.eval()\n",
    "    gradients = []\n",
    "\n",
    "    input_gene_ids = input_gene_ids.to(device)\n",
    "    input_values = input_values.to(device)\n",
    "    ageGroup = ageGroup.to(device)\n",
    "    \n",
    "    input_values.requires_grad =True\n",
    "\n",
    "    # Mask for padding tokens\n",
    "    src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # 添加噪声并确保 `requires_grad=True`，以便在反向传播时计算梯度\n",
    "        noisy_input_values = input_values + noise_level * torch.randn_like(input_values)\n",
    "        # noisy_input_values.requires_grad = True  # 确保启用梯度追踪\n",
    "\n",
    "        print(f\"noisy_input_values: {noisy_input_values}\")\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                noisy_input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=False, \n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            output_values = output_dict[\"classified_output\"]\n",
    "\n",
    "            print(f\"Output values: {output_values}\")\n",
    "\n",
    "            # 计算损失并进行反向传播\n",
    "            loss = criterion_cls(output_values, ageGroup)\n",
    "            print(f\"Loss: {loss}\")\n",
    "            \n",
    "            model.zero_grad()  # 清除之前的梯度\n",
    "            loss.backward(retain_graph=True)  # 保留计算图\n",
    "\n",
    "            # 检查梯度是否被正确计算\n",
    "            if noisy_input_values.grad is None:\n",
    "                print(\"No gradient for noisy_input_values\")\n",
    "            else:\n",
    "                gradients.append(noisy_input_values.grad.cpu().numpy())\n",
    "\n",
    "    # 平均多个样本的梯度\n",
    "    avg_gradients = np.mean(gradients, axis=0)\n",
    "    \n",
    "    return avg_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy_input_values: tensor([[ 9.5203e-02,  4.1134e+01,  8.9827e+00,  ...,  6.6870e+01,\n",
      "          1.0117e+02,  1.0100e+02],\n",
      "        [ 1.5043e-01,  8.1013e+01,  9.4984e+01,  ...,  1.0111e+02,\n",
      "          1.0104e+02,  1.0105e+02],\n",
      "        [ 4.6320e-02,  4.6037e+01,  6.0034e+01,  ...,  1.0097e+02,\n",
      "          1.0098e+02,  1.0105e+02],\n",
      "        ...,\n",
      "        [ 5.8184e-02,  4.5268e+01,  7.0048e+01,  ...,  1.0102e+02,\n",
      "          1.0096e+02,  1.0098e+02],\n",
      "        [ 2.0101e-02,  4.2999e+01,  8.4075e+01,  ...,  7.0994e+01,\n",
      "          1.0090e+02,  1.0091e+02],\n",
      "        [-3.7871e-02,  2.3959e+01,  4.2864e+01,  ...,  1.7879e+01,\n",
      "          4.9921e+01,  1.0109e+02]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacty of 79.15 GiB of which 684.12 MiB is free. Including non-PyTorch memory, this process has 78.46 GiB memory in use. Of the allocated memory 75.57 GiB is allocated by PyTorch, and 2.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m ageGroup \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mageGroup\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 使用 SmoothGrad 方法计算基因重要性\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m avg_gradients \u001b[38;5;241m=\u001b[39m \u001b[43msmooth_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mageGroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 可视化平均梯度值，显示哪些基因对预测最重要\u001b[39;00m\n\u001b[1;32m     19\u001b[0m gene_names \u001b[38;5;241m=\u001b[39m adata\u001b[38;5;241m.\u001b[39mvar[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36msmooth_grad\u001b[0;34m(model, input_gene_ids, input_values, ageGroup, vocab, pad_token, noise_level, num_samples)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoisy_input_values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoisy_input_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m---> 23\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoisy_input_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCLS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCCE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMVC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mECS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     output_values \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassified_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/project/code/ageClassification/scgpt/model/model.py:415\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# transformer_output: (batch, seq_len, embsize)\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# if self.use_batch_labels:\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m#     batch_emb = self.batch_encoder(batch_labels)  # (batch, embsize)\u001b[39;00m\n\u001b[1;32m    422\u001b[0m output \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/data/mr423/project/code/ageClassification/scgpt/model/model.py:247\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    244\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(total_embs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# total_embs: (batch, seq_len, d_model)   \u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:389\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# attention_weights_list = []  # 保存每层的注意力权重\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 389\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;66;03m# output, attention_weights = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers) # add\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# print(f\"attention_weights in TE {attention_weights}\")\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# if attention_weights is not None:\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m#     attention_weights_list.append(attention_weights)\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/project/code/ageClassification/scgpt/model/model.py:792\u001b[0m, in \u001b[0;36mFlashTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(src2)\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 792\u001b[0m     src2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    793\u001b[0m     src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(src2)\n\u001b[1;32m    794\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(src)\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/flash_attn/flash_attention.py:117\u001b[0m, in \u001b[0;36mFlashMHA.forward\u001b[0;34m(self, x, key_padding_mask, need_weights)\u001b[0m\n\u001b[1;32m    115\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWqkv(x)\n\u001b[1;32m    116\u001b[0m qkv \u001b[38;5;241m=\u001b[39m rearrange(qkv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s (three h d) -> b s three h d\u001b[39m\u001b[38;5;124m'\u001b[39m, three\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[0;32m--> 117\u001b[0m context, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(rearrange(context, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s h d -> b s (h d)\u001b[39m\u001b[38;5;124m'\u001b[39m)), attn_weights\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/flash_attn/flash_attention.py:63\u001b[0m, in \u001b[0;36mFlashAttention.forward\u001b[0;34m(self, qkv, key_padding_mask, causal, cu_seqlens, max_s, need_weights)\u001b[0m\n\u001b[1;32m     61\u001b[0m nheads \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(qkv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s three h d -> b s (three h d)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m x_unpad, indices, cu_seqlens, max_s \u001b[38;5;241m=\u001b[39m \u001b[43munpad_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m x_unpad \u001b[38;5;241m=\u001b[39m rearrange(x_unpad, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnz (three h d) -> nnz three h d\u001b[39m\u001b[38;5;124m'\u001b[39m, three\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39mnheads)\n\u001b[1;32m     65\u001b[0m output_unpad \u001b[38;5;241m=\u001b[39m flash_attn_unpadded_qkvpacked_func(\n\u001b[1;32m     66\u001b[0m     x_unpad, cu_seqlens, max_s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_p \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     67\u001b[0m     softmax_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_scale, causal\u001b[38;5;241m=\u001b[39mcausal\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/flash_attn/bert_padding.py:116\u001b[0m, in \u001b[0;36munpad_input\u001b[0;34m(hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    110\u001b[0m cu_seqlens \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(torch\u001b[38;5;241m.\u001b[39mcumsum(seqlens_in_batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# TD [2022-03-04] We don't want to index with a bool mask, because Pytorch will expand the\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# bool mask, then call nonzero to get the indices, then index with those. The indices is @dim\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# times larger than it needs to be, wasting memory. It's faster and more memory-efficient to\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# index with integer indices. Moreover, torch's index is a bit slower than it needs to be,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# so we write custom forward and backward to make it a bit faster.\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mindex_first_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb s ... -> (b s) ...\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m, indices,\n\u001b[1;32m    117\u001b[0m         cu_seqlens, max_seqlen_in_batch)\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/flash_attn/bert_padding.py:19\u001b[0m, in \u001b[0;36mIndexFirstAxis.forward\u001b[0;34m(ctx, input, indices)\u001b[0m\n\u001b[1;32m     16\u001b[0m second_dim \u001b[38;5;241m=\u001b[39m other_shape\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# return input[indices]\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb ... -> b (...)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz -> z d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mother_shape)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacty of 79.15 GiB of which 684.12 MiB is free. Including non-PyTorch memory, this process has 78.46 GiB memory in use. Of the allocated memory 75.57 GiB is allocated by PyTorch, and 2.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "for batch_data in test_loader:\n",
    "    input_gene_ids = batch_data[\"gene_ids\"]\n",
    "    input_values = batch_data[\"values\"]\n",
    "    ageGroup = batch_data[\"ageGroup\"]\n",
    "    \n",
    "    # 使用 SmoothGrad 方法计算基因重要性\n",
    "    avg_gradients = smooth_grad(\n",
    "        model,\n",
    "        input_gene_ids,\n",
    "        input_values,\n",
    "        ageGroup,\n",
    "        vocab,\n",
    "        pad_token=pad_token,\n",
    "        noise_level=0.1,\n",
    "        num_samples=50\n",
    "    )\n",
    "    \n",
    "    # 可视化平均梯度值，显示哪些基因对预测最重要\n",
    "    gene_names = adata.var[\"gene_name\"].tolist()\n",
    "    important_genes = np.mean(np.abs(avg_gradients), axis=0)  # 对不同样本取平均值\n",
    "    \n",
    "    sorted_indices = np.argsort(-important_genes)  # 由大到小排序\n",
    "    top_genes = [gene_names[i] for i in sorted_indices[:20]]  # 获取最重要的前20个基因\n",
    "    top_importances = important_genes[sorted_indices[:20]]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_importances, y=top_genes)\n",
    "    plt.xlabel(\"Not importance Score\")\n",
    "    plt.ylabel(\"Plasma protein-encoding genes\")\n",
    "    # plt.title(\"Top 20 not important Genes for Age Prediction\")\n",
    "\n",
    "    output_path = \"./top_no_important_genes_smoothgrad.png\"  # 你可以修改保存路径和文件名\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")  # dpi 300确保高质量保存，bbox_inches=\"tight\" 去除空白边框\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    break  # 只处理一个批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
