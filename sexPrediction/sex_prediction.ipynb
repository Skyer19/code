{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated temp directory: /data/mr423/tmp\n",
      "scgpt location:  /data/mr423/project/code/sexPrediction/scgpt/__init__.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'scgpt' from '/data/mr423/project/code/sexPrediction/scgpt/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 检查 tempfile 模块使用的临时文件目录\n",
    "temp_dir = tempfile.gettempdir()\n",
    "print(\"Updated temp directory:\", temp_dir)\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "new_path = '/data/mr423/project/sexPrediction/'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "# relaod the scgpt files\n",
    "import scgpt\n",
    "print(\"scgpt location: \", scgpt.__file__)\n",
    "importlib.reload(scgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"WANDB_MODE\"]= \"offline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'do_train': True, 'load_model': '/data/mr423/project/pre_trained_model/scGPT_human', 'n_bins': 101, 'epochs': 50, 'lr': 0.001, 'batch_size': 128, 'layer_size': 128, 'nlayers': 4, 'nhead': 8, 'dropout': 0.0, 'use_fast_transformer': True, 'pre_norm': False, 'amp': True, 'freeze': True}\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Settings for wandb mentior\n",
    "######################################################################\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    do_train=True,\n",
    "    load_model=\"/data/mr423/project/pre_trained_model/scGPT_human\",\n",
    "    n_bins=101,\n",
    "\n",
    "    epochs=50,\n",
    "    lr=0.001,\n",
    "    batch_size=128,\n",
    "\n",
    "    layer_size=128, # 128\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=8,  # number of heads in nn.MultiheadAttention\n",
    "    \n",
    "    dropout=0.0,  # dropout probability\n",
    "\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    freeze = True, #freeze\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"age_pred\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for input and preprocessing\n",
    "######################################################################\n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"                                    # decide the type of the input\n",
    "\n",
    "input_emb_style = \"category\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for optimizer\n",
    "######################################################################\n",
    "lr = config.lr \n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "early_stop = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for the model\n",
    "######################################################################\n",
    "use_fast_transformer = config.use_fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "\n",
    "# if input_style == \"binned\":\n",
    "#     if input_emb_style == \"scaling\":\n",
    "#         raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "# elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "#     if input_emb_style == \"category\":\n",
    "#         raise ValueError(\n",
    "#             \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "#         )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to /data/mr423/project/code/save/biobank-Aug23-20-05\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Settings for the running recording\n",
    "######################################################################\n",
    "dataset_name = 'biobank'\n",
    "save_dir = Path(f\"/data/mr423/project/code/save/{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37304, 2919)\n",
      "(4145, 2919)\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Data loading\n",
    "######################################################################\n",
    "adata = sc.read(\"/data/mr423/project/data/3-OLINK_data_train_withOutlier_all.h5ad\")\n",
    "adata_test = sc.read(\"/data/mr423/project/data/3-OLINK_data_test_withOutlier_all.h5ad\")\n",
    "\n",
    "print(adata.shape)\n",
    "print(adata_test.shape)\n",
    "\n",
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\" \n",
    "\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "\n",
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "adata_test_raw = adata_test.copy()\n",
    "adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "\n",
    "\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2895/2919 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /data/mr423/project/pre_trained_model/scGPT_human/best_model.pt, the model args will override the config /data/mr423/project/pre_trained_model/scGPT_human/args.json.\n",
      "\n",
      "**** parameters from the pre-trained model ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** parameters from the pre-trained model ****\n",
      "\n",
      "**** actual model parameters ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** actual model parameters ****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The pre-trained model\n",
    "######################################################################\n",
    "if config.load_model is not None:\n",
    "    model_dir = config.load_model\n",
    "    model_config_file = model_dir + \"/args.json\"\n",
    "    model_file = model_dir + \"/best_model.pt\"\n",
    "    vocab_file = model_dir + \"/vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "    print(\"\\n**** parameters from the pre-trained model ****\")\n",
    "    print(f'layer_size = embsize: {model_configs[\"embsize\"]} = d_hid: {model_configs[\"d_hid\"]}, n_layers: {model_configs[\"nlayers\"]}, nhead: {model_configs[\"nheads\"]}')\n",
    "    print(\"**** parameters from the pre-trained model ****\\n\")\n",
    "\n",
    "    print(\"**** actual model parameters ****\")\n",
    "    print(f'layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "    print(\"**** actual model parameters ****\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "######################################################################\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=3000,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>DoB_Year</th>\n",
       "      <th>DoB_Month</th>\n",
       "      <th>DoB_Day</th>\n",
       "      <th>DoB</th>\n",
       "      <th>Date_Attend</th>\n",
       "      <th>age</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>str_batch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2144829-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1939</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1939-01-15</td>\n",
       "      <td>2007-11-16</td>\n",
       "      <td>68.835044</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154285-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-01-15</td>\n",
       "      <td>2007-07-20</td>\n",
       "      <td>62.507871</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679423-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1945</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-11-15</td>\n",
       "      <td>2009-05-19</td>\n",
       "      <td>63.507187</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172610-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1941-12-15</td>\n",
       "      <td>2009-09-23</td>\n",
       "      <td>67.772758</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011532-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1954</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1954-01-15</td>\n",
       "      <td>2009-10-20</td>\n",
       "      <td>55.761807</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137580-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1957</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1957-05-15</td>\n",
       "      <td>2007-08-14</td>\n",
       "      <td>50.247775</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378384-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1950</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1950-04-15</td>\n",
       "      <td>2009-06-27</td>\n",
       "      <td>59.200548</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220136-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1945</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-06-15</td>\n",
       "      <td>2008-11-11</td>\n",
       "      <td>63.408624</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988172-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1956</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1956-01-15</td>\n",
       "      <td>2010-07-13</td>\n",
       "      <td>54.491444</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5677442-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1963</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1963-05-15</td>\n",
       "      <td>2010-06-03</td>\n",
       "      <td>47.052704</td>\n",
       "      <td>40-50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37304 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex  DoB_Year  DoB_Month  DoB_Day         DoB Date_Attend  \\\n",
       "Id                                                                     \n",
       "2144829-0    0      1939          1       15  1939-01-15  2007-11-16   \n",
       "3154285-0    0      1945          1       15  1945-01-15  2007-07-20   \n",
       "1679423-0    1      1945         11       15  1945-11-15  2009-05-19   \n",
       "1172610-0    1      1941         12       15  1941-12-15  2009-09-23   \n",
       "4011532-0    1      1954          1       15  1954-01-15  2009-10-20   \n",
       "...        ...       ...        ...      ...         ...         ...   \n",
       "1137580-0    1      1957          5       15  1957-05-15  2007-08-14   \n",
       "3378384-0    0      1950          4       15  1950-04-15  2009-06-27   \n",
       "1220136-0    0      1945          6       15  1945-06-15  2008-11-11   \n",
       "4988172-0    0      1956          1       15  1956-01-15  2010-07-13   \n",
       "5677442-0    1      1963          5       15  1963-05-15  2010-06-03   \n",
       "\n",
       "                 age Age_Group  batch_id str_batch  \n",
       "Id                                                  \n",
       "2144829-0  68.835044     60-70         0         0  \n",
       "3154285-0  62.507871     60-70         0         0  \n",
       "1679423-0  63.507187     60-70         0         0  \n",
       "1172610-0  67.772758     60-70         0         0  \n",
       "4011532-0  55.761807     50-60         0         0  \n",
       "...              ...       ...       ...       ...  \n",
       "1137580-0  50.247775     50-60         0         0  \n",
       "3378384-0  59.200548     50-60         0         0  \n",
       "1220136-0  63.408624     60-70         0         0  \n",
       "4988172-0  54.491444     50-60         0         0  \n",
       "5677442-0  47.052704     40-50         0         0  \n",
       "\n",
       "[37304 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_key:  X_binned\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Split the data to train and test\n",
    "######################################################################\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "\n",
    "print(\"input_layer_key: \", input_layer_key)\n",
    "\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "sex = adata.obs[\"sex\"].tolist()\n",
    "sex = np.array(sex)\n",
    "\n",
    "# print(age)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_sex,\n",
    "    valid_sex,\n",
    ") = train_test_split(\n",
    "    all_counts, sex, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 29843, \n",
      "\t feature length: 2896\n",
      "scGPT - INFO - valid set number of samples: 7461, \n",
      "\t feature length: 2896\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Tokenize the data\n",
    "######################################################################\n",
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_data() -> Tuple[Dict[str, torch.Tensor]]:\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    \n",
    "    input_values_train, input_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_sex_train = torch.from_numpy(train_sex).float()\n",
    "    tensor_sex_valid = torch.from_numpy(valid_sex).float()\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"sex\": tensor_sex_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"sex\": tensor_sex_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " **** load model parameters ****\n",
      "lr = 0.001, batch_size = 128, epochs = 50\n",
      "ntokens = 60697, layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** load model parameters ****\n",
      "\n",
      "name: encoder.embedding.weight\n",
      "freezing weights for: encoder.embedding.weight\n",
      "name: encoder.enc_norm.weight\n",
      "freezing weights for: encoder.enc_norm.weight\n",
      "name: encoder.enc_norm.bias\n",
      "freezing weights for: encoder.enc_norm.bias\n",
      "name: value_encoder.embedding.weight\n",
      "freezing weights for: value_encoder.embedding.weight\n",
      "name: value_encoder.enc_norm.weight\n",
      "freezing weights for: value_encoder.enc_norm.weight\n",
      "name: value_encoder.enc_norm.bias\n",
      "freezing weights for: value_encoder.enc_norm.bias\n",
      "name: transformer_encoder.layers.0.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.0.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.weight\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.bias\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.weight\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.bias\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.weight\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.bias\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.weight\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.bias\n",
      "name: transformer_encoder.layers.1.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.1.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.weight\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.bias\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.weight\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.bias\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.weight\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.bias\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.weight\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.bias\n",
      "name: transformer_encoder.layers.2.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.2.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.weight\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.bias\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.weight\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.bias\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.weight\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.bias\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.weight\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.bias\n",
      "name: transformer_encoder.layers.3.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.3.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.weight\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.bias\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.weight\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.bias\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.weight\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.bias\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.weight\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.bias\n",
      "name: transformer_encoder.layers.4.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.4.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.weight\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.bias\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.weight\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.bias\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.weight\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.bias\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.weight\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.bias\n",
      "name: transformer_encoder.layers.5.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.5.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.weight\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.bias\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.weight\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.bias\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.weight\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.bias\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.weight\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.bias\n",
      "name: transformer_encoder.layers.6.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.6.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.weight\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.bias\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.weight\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.bias\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.weight\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.bias\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.weight\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.bias\n",
      "name: transformer_encoder.layers.7.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.7.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.weight\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.bias\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.weight\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.bias\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.weight\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.bias\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.weight\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.bias\n",
      "name: transformer_encoder.layers.8.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.8.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.weight\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.bias\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.weight\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.bias\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.weight\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.bias\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.weight\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.bias\n",
      "name: transformer_encoder.layers.9.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.9.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.weight\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.bias\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.weight\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.bias\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.weight\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.bias\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.weight\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.bias\n",
      "name: transformer_encoder.layers.10.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.10.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.weight\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.bias\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.weight\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.bias\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.weight\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.bias\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.weight\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.bias\n",
      "name: transformer_encoder.layers.11.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.11.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.weight\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.bias\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.weight\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.bias\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.weight\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.bias\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.weight\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.bias\n",
      "name: linear.weight\n",
      "name: linear.bias\n",
      "name: reg_decoder.fc1.weight\n",
      "name: reg_decoder.fc1.bias\n",
      "name: reg_decoder.bn1.weight\n",
      "name: reg_decoder.bn1.bias\n",
      "name: reg_decoder.fc2.weight\n",
      "name: reg_decoder.fc2.bias\n",
      "name: reg_decoder.bn2.weight\n",
      "name: reg_decoder.bn2.bias\n",
      "name: reg_decoder.fc4.weight\n",
      "name: reg_decoder.fc4.bias\n",
      "scGPT - INFO - Total Pre freeze Params 50725889\n",
      "scGPT - INFO - Total Post freeze Params 658433\n",
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): CategoryValueEncoder(\n",
      "    (embedding): Embedding(103, 512, padding_idx=101)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (reg_decoder): RegressionEncoder(\n",
      "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation1): LeakyReLU(negative_slope=0.01)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation2): LeakyReLU(negative_slope=0.01)\n",
      "    (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load the model\n",
    "######################################################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of gene vocabulary\n",
    "\n",
    "print(\"\\n\\n **** load model parameters ****\")\n",
    "print(f'lr = {lr}, batch_size = {batch_size}, epochs = {epochs}')\n",
    "print(f'ntokens = {ntokens}, layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "print(\"**** load model parameters ****\\n\")\n",
    "\n",
    "model = TransformerModel(\n",
    "    ntokens,    # size of gene vocabulary\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=False,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    num_batch_labels=None,\n",
    "    domain_spec_batchnorm=False,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    explicit_zero_prob=False,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        # for k, v in pretrained_dict.items():\n",
    "            # logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        \n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    # print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    # if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Loss function\n",
    "######################################################################\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# nn.SmoothL1Loss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (0.9, 0.999))\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Train the model\n",
    "######################################################################\n",
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_num = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        \n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)        # torch.Size([batch_size, 2890]) -- (batch_size, seq_len)\n",
    "        input_values = batch_data[\"values\"].to(device)            # torch.Size([batch_size, 2890]) -- (batch_size, seq_len)\n",
    "        sex = batch_data[\"sex\"].to(device)\n",
    "\n",
    "        # print(\"input_gene_ids shape: \", input_gene_ids.shape)\n",
    "\n",
    "        '''\n",
    "        src_key_padding_mask是一个布尔型张量,形状与 input_gene_ids 相同，即 (batch_size, seq_len)。\n",
    "        它指示哪些位置是填充的True, 哪些位置是有效的输入 False \n",
    "        \n",
    "        input_gene_ids.eq(vocab[pad_token]) 将会对每个位置检查是否等于 pad_token 的索引。如果是，返回 True, 否则返回 False\n",
    "\n",
    "        在 Transformer 模型中用于防止填充部分影响模型的学习过程。通过标记填充位置，模型在进行自注意力计算时可以忽略这些无意义的部分\n",
    "        '''\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=False,\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=False,\n",
    "                #generative_training=False\n",
    "            )\n",
    "            \n",
    "            loss = 0.0\n",
    "            output_values = output_dict[\"reg_output\"]\n",
    "            output_values = output_values.squeeze() \n",
    "            loss = criterion(output_values, sex)\n",
    "\n",
    "            # print(\"output : \",output_values.size())\n",
    "            # print(\"ground : \",age.size())\n",
    "                       \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()  # 缩放损失并反向传播\n",
    "        scaler.unscale_(optimizer)  # 还原梯度到原始尺度\n",
    "        scaler.step(optimizer)  # 调用优化器的step方法\n",
    "        scaler.update()  # 更新缩放因子\n",
    "\n",
    "        # with warnings.catch_warnings(record=True) as w:\n",
    "        #     warnings.filterwarnings(\"always\")\n",
    "        #     torch.nn.utils.clip_grad_norm_(\n",
    "        #         model.parameters(),\n",
    "        #         1.0,\n",
    "        #         error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "        #     )\n",
    "        #     if len(w) > 0:\n",
    "        #         logger.warning(\n",
    "        #             f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "        #             f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "        #             \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "        #         )\n",
    "    \n",
    "        \n",
    "        total_loss += loss.item() * len(input_gene_ids)\n",
    "        total_num += len(input_gene_ids)\n",
    "\n",
    "        # 保存预测值和真实值以计算其他评估指标\n",
    "        all_preds.append(output_values.cpu())\n",
    "        all_targets.append(sex.cpu())\n",
    "        \n",
    "    epoch_loss = total_loss / total_num  # 计算整个epoch的平均损失\n",
    "\n",
    "    # 将所有批次的预测值和真实值连接起来\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    \n",
    "    # 定义评估指标函数\n",
    "    mse = torch.mean((all_preds - all_targets) ** 2)\n",
    "    mae = torch.mean(torch.abs(all_preds - all_targets))\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    ss_res = torch.sum((all_targets - all_preds) ** 2)\n",
    "    ss_tot = torch.sum((all_targets - torch.mean(all_targets)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "    mape = torch.mean(torch.abs((all_targets - all_preds) / all_targets)) * 100\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "    # print(f\"Epoch {epoch}/{epochs}, Epoch Loss: {epoch_loss:.4f}\")   \n",
    "\n",
    "    logger.info(\n",
    "    f\"| train | total_num {total_num} | epoch loss {epoch_loss:5.4f} | mse {mse:5.4f} | \" \n",
    "    f\" mae {mae:5.4f} | rmse {rmse:5.4f} | \" \n",
    "    f\" r2 {r2:5.4f} | mape {mape:5.4f}\")\n",
    "    \n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train/loss\": epoch_loss,\n",
    "            \"train/mse\": mse,\n",
    "            \"train/mae\": mae,\n",
    "            \"train/rmse\": rmse,\n",
    "            \"train/r2\": r2,\n",
    "            \"train/mape\": mape,\n",
    "            \"train/r2/mae\": r2/mae,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
