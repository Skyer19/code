{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d246dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated temp directory: /vol/bitbucket/mr423/tmp\n",
      "['../sexPrediction/', '/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python39.zip', '/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9', '/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/lib-dynload', '', '/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages']\n",
      "scgpt location:  /vol/bitbucket/mr423/project/code/sexPrediction/../sexPrediction/scgpt/__init__.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'scgpt' from '/vol/bitbucket/mr423/project/code/sexPrediction/../sexPrediction/scgpt/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 检查 tempfile 模块使用的临时文件目录\n",
    "temp_dir = tempfile.gettempdir()\n",
    "print(\"Updated temp directory:\", temp_dir)\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "new_path = '../sexPrediction/'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "# relaod the scgpt files\n",
    "import scgpt\n",
    "print(\"scgpt location: \", scgpt.__file__)\n",
    "importlib.reload(scgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb815e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 25 16:43:33 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:2D:00.0  On |                  N/A |\n",
      "| 30%   56C    P2             164W / 320W |   7849MiB / 16376MiB |     67%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1527895      C   /bin/python                                7226MiB |\n",
      "|    0   N/A  N/A   2051147      G   /usr/lib/xorg/Xorg                           85MiB |\n",
      "|    0   N/A  N/A   3543561      C   python                                      520MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3daceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"WANDB_MODE\"]= \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c090665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'do_train': True, 'load_model': '/vol/bitbucket/mr423/project/pre_trained_model/scGPT_human', 'n_bins': 101, 'epochs': 2, 'lr': 0.001, 'batch_size': 128, 'layer_size': 16, 'nlayers': 4, 'nhead': 8, 'dropout': 0.0, 'use_fast_transformer': True, 'pre_norm': False, 'amp': True, 'freeze': True}\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Settings for wandb mentior\n",
    "######################################################################\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    do_train=True,\n",
    "    load_model=\"/vol/bitbucket/mr423/project/pre_trained_model/scGPT_human\",\n",
    "    n_bins=101,\n",
    "\n",
    "    epochs=2, # 2 !!!!!!!!!!!!  test only\n",
    "    lr=0.001,\n",
    "    batch_size=128,   # 128 !!!!!!!!!!!!  test only\n",
    "\n",
    "    layer_size=16, # 128\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=8,  # number of heads in nn.MultiheadAttention\n",
    "    \n",
    "    dropout=0.0,  # dropout probability\n",
    "\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    freeze = True, #freeze\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"gender_pred\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ceb2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for input and preprocessing\n",
    "######################################################################\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "input_emb_style = \"category\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "\n",
    "\n",
    "# settings for training\n",
    "CLS = True  # celltype classification objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1832cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for optimizer\n",
    "######################################################################\n",
    "lr = config.lr\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "early_stop = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3214e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for the model\n",
    "######################################################################\n",
    "use_fast_transformer = config.use_fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "\n",
    "\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891a7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Validate the settings\n",
    "######################################################################\n",
    "# assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "# assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25227609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to record/dev_biobank-gender_pred-Aug25-16-44\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Settings for the running recording\n",
    "######################################################################\n",
    "dataset_name = 'biobank-gender_pred'\n",
    "save_dir = Path(f\"./record/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7c7a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37304, 2919)\n",
      "(4145, 2919)\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Data loading\n",
    "######################################################################\n",
    "adata = sc.read(\"/vol/bitbucket/mr423/project/data/3-OLINK_data_train_withOutlier_all.h5ad\")\n",
    "adata_test = sc.read(\"/vol/bitbucket/mr423/project/data/3-OLINK_data_test_withOutlier_all.h5ad\")\n",
    "\n",
    "print(adata.shape)\n",
    "print(adata_test.shape)\n",
    "\n",
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\" \n",
    "\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "\n",
    "data_is_raw = False\n",
    "\n",
    "adata_test_raw = adata_test.copy()\n",
    "adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47134c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>DoB_Year</th>\n",
       "      <th>DoB_Month</th>\n",
       "      <th>DoB_Day</th>\n",
       "      <th>DoB</th>\n",
       "      <th>Date_Attend</th>\n",
       "      <th>age</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>str_batch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2144829-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1939</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1939-01-15</td>\n",
       "      <td>2007-11-16</td>\n",
       "      <td>68.835044</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154285-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-01-15</td>\n",
       "      <td>2007-07-20</td>\n",
       "      <td>62.507871</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679423-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1945</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-11-15</td>\n",
       "      <td>2009-05-19</td>\n",
       "      <td>63.507187</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172610-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1941-12-15</td>\n",
       "      <td>2009-09-23</td>\n",
       "      <td>67.772758</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011532-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1954</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1954-01-15</td>\n",
       "      <td>2009-10-20</td>\n",
       "      <td>55.761807</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503594-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1947</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1947-02-15</td>\n",
       "      <td>2009-08-13</td>\n",
       "      <td>62.491444</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494250-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1945</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-12-15</td>\n",
       "      <td>2009-11-10</td>\n",
       "      <td>63.904175</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746191-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1951</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1951-12-15</td>\n",
       "      <td>2009-06-26</td>\n",
       "      <td>57.530459</td>\n",
       "      <td>50-60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342815-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1942</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1942-08-15</td>\n",
       "      <td>2008-03-11</td>\n",
       "      <td>65.571526</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6020978-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1947</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1947-12-15</td>\n",
       "      <td>2009-07-13</td>\n",
       "      <td>61.577002</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41449 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex  DoB_Year  DoB_Month  DoB_Day         DoB Date_Attend  \\\n",
       "Id                                                                     \n",
       "2144829-0    0      1939          1       15  1939-01-15  2007-11-16   \n",
       "3154285-0    0      1945          1       15  1945-01-15  2007-07-20   \n",
       "1679423-0    1      1945         11       15  1945-11-15  2009-05-19   \n",
       "1172610-0    1      1941         12       15  1941-12-15  2009-09-23   \n",
       "4011532-0    1      1954          1       15  1954-01-15  2009-10-20   \n",
       "...        ...       ...        ...      ...         ...         ...   \n",
       "2503594-1    0      1947          2       15  1947-02-15  2009-08-13   \n",
       "3494250-1    1      1945         12       15  1945-12-15  2009-11-10   \n",
       "5746191-1    1      1951         12       15  1951-12-15  2009-06-26   \n",
       "4342815-1    0      1942          8       15  1942-08-15  2008-03-11   \n",
       "6020978-1    1      1947         12       15  1947-12-15  2009-07-13   \n",
       "\n",
       "                 age Age_Group batch_id str_batch  \n",
       "Id                                                 \n",
       "2144829-0  68.835044     60-70        0         0  \n",
       "3154285-0  62.507871     60-70        0         0  \n",
       "1679423-0  63.507187     60-70        0         0  \n",
       "1172610-0  67.772758     60-70        0         0  \n",
       "4011532-0  55.761807     50-60        0         0  \n",
       "...              ...       ...      ...       ...  \n",
       "2503594-1  62.491444     60-70        1         1  \n",
       "3494250-1  63.904175     60-70        1         1  \n",
       "5746191-1  57.530459     50-60        1         1  \n",
       "4342815-1  65.571526     60-70        1         1  \n",
       "6020978-1  61.577002     60-70        1         1  \n",
       "\n",
       "[41449 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7c8944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "\n",
    "gender_id_labels = adata.obs[\"sex\"].astype(\"category\").cat.codes.values\n",
    "# ageGroup_types = adata.obs[\"Age_Group\"].unique()\n",
    "adata.obs[\"gender_id\"] = gender_id_labels\n",
    "\n",
    "\n",
    "n_cls = len(np.unique(gender_id_labels))\n",
    "\n",
    "id2type = dict(enumerate(adata.obs[\"sex\"].astype(\"category\").cat.categories))\n",
    "print(id2type)\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d8944aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>DoB_Year</th>\n",
       "      <th>DoB_Month</th>\n",
       "      <th>DoB_Day</th>\n",
       "      <th>DoB</th>\n",
       "      <th>Date_Attend</th>\n",
       "      <th>age</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>str_batch</th>\n",
       "      <th>gender_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2144829-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1939</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1939-01-15</td>\n",
       "      <td>2007-11-16</td>\n",
       "      <td>68.835044</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154285-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-01-15</td>\n",
       "      <td>2007-07-20</td>\n",
       "      <td>62.507871</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679423-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1945</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-11-15</td>\n",
       "      <td>2009-05-19</td>\n",
       "      <td>63.507187</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172610-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1941-12-15</td>\n",
       "      <td>2009-09-23</td>\n",
       "      <td>67.772758</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011532-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1954</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1954-01-15</td>\n",
       "      <td>2009-10-20</td>\n",
       "      <td>55.761807</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503594-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1947</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1947-02-15</td>\n",
       "      <td>2009-08-13</td>\n",
       "      <td>62.491444</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494250-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1945</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1945-12-15</td>\n",
       "      <td>2009-11-10</td>\n",
       "      <td>63.904175</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746191-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1951</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1951-12-15</td>\n",
       "      <td>2009-06-26</td>\n",
       "      <td>57.530459</td>\n",
       "      <td>50-60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342815-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1942</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1942-08-15</td>\n",
       "      <td>2008-03-11</td>\n",
       "      <td>65.571526</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6020978-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1947</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1947-12-15</td>\n",
       "      <td>2009-07-13</td>\n",
       "      <td>61.577002</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41449 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex  DoB_Year  DoB_Month  DoB_Day         DoB Date_Attend  \\\n",
       "Id                                                                     \n",
       "2144829-0    0      1939          1       15  1939-01-15  2007-11-16   \n",
       "3154285-0    0      1945          1       15  1945-01-15  2007-07-20   \n",
       "1679423-0    1      1945         11       15  1945-11-15  2009-05-19   \n",
       "1172610-0    1      1941         12       15  1941-12-15  2009-09-23   \n",
       "4011532-0    1      1954          1       15  1954-01-15  2009-10-20   \n",
       "...        ...       ...        ...      ...         ...         ...   \n",
       "2503594-1    0      1947          2       15  1947-02-15  2009-08-13   \n",
       "3494250-1    1      1945         12       15  1945-12-15  2009-11-10   \n",
       "5746191-1    1      1951         12       15  1951-12-15  2009-06-26   \n",
       "4342815-1    0      1942          8       15  1942-08-15  2008-03-11   \n",
       "6020978-1    1      1947         12       15  1947-12-15  2009-07-13   \n",
       "\n",
       "                 age Age_Group  batch_id str_batch  gender_id  \n",
       "Id                                                             \n",
       "2144829-0  68.835044     60-70         0         0          0  \n",
       "3154285-0  62.507871     60-70         0         0          0  \n",
       "1679423-0  63.507187     60-70         0         0          1  \n",
       "1172610-0  67.772758     60-70         0         0          1  \n",
       "4011532-0  55.761807     50-60         0         0          1  \n",
       "...              ...       ...       ...       ...        ...  \n",
       "2503594-1  62.491444     60-70         1         1          0  \n",
       "3494250-1  63.904175     60-70         1         1          1  \n",
       "5746191-1  57.530459     50-60         1         1          1  \n",
       "4342815-1  65.571526     60-70         1         1          0  \n",
       "6020978-1  61.577002     60-70         1         1          1  \n",
       "\n",
       "[41449 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eef4849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2895/2919 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /vol/bitbucket/mr423/project/pre_trained_model/scGPT_human/best_model.pt, the model args will override the config /vol/bitbucket/mr423/project/pre_trained_model/scGPT_human/args.json.\n",
      "\n",
      "**** parameters from the pre-trained model ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** parameters from the pre-trained model ****\n",
      "\n",
      "**** actual model parameters ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** actual model parameters ****\n",
      "\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The pre-trained model\n",
    "######################################################################\n",
    "if config.load_model is not None:\n",
    "    model_dir = config.load_model\n",
    "    model_config_file = model_dir + \"/args.json\"\n",
    "    model_file = model_dir + \"/best_model.pt\"\n",
    "    vocab_file = model_dir + \"/vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "    print(\"\\n**** parameters from the pre-trained model ****\")\n",
    "    print(f'layer_size = embsize: {model_configs[\"embsize\"]} = d_hid: {model_configs[\"d_hid\"]}, n_layers: {model_configs[\"nlayers\"]}, nhead: {model_configs[\"nheads\"]}')\n",
    "    print(\"**** parameters from the pre-trained model ****\\n\")\n",
    "\n",
    "    print(\"**** actual model parameters ****\")\n",
    "    print(f'layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "    print(\"**** actual model parameters ****\\n\")\n",
    "\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=False,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=3000,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b9e939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Split the data to train and test\n",
    "######################################################################\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "gender_labels = adata.obs[\"gender_id\"].tolist()  # make sure count from 0\n",
    "gender_labels = np.array(gender_labels)\n",
    "\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_gender,\n",
    "    valid_gender,\n",
    ") = train_test_split(\n",
    "    all_counts, gender_labels, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9cd1e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 29843, \n",
      "\t feature length: 2896\n",
      "scGPT - INFO - valid set number of samples: 7461, \n",
      "\t feature length: 2896\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Tokenize the data\n",
    "######################################################################\n",
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "\n",
    "    input_values_train, input_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_gender_train = torch.from_numpy(train_gender).long()\n",
    "    tensor_gender_valid = torch.from_numpy(valid_gender).long()\n",
    "\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"gender\": tensor_gender_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"gender\": tensor_gender_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a40ab9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading SOME model params from /vol/bitbucket/mr423/project/pre_trained_model/scGPT_human/best_model.pt\n",
      "name: encoder.embedding.weight\n",
      "freezing weights for: encoder.embedding.weight\n",
      "name: encoder.enc_norm.weight\n",
      "freezing weights for: encoder.enc_norm.weight\n",
      "name: encoder.enc_norm.bias\n",
      "freezing weights for: encoder.enc_norm.bias\n",
      "name: value_encoder.embedding.weight\n",
      "freezing weights for: value_encoder.embedding.weight\n",
      "name: value_encoder.enc_norm.weight\n",
      "freezing weights for: value_encoder.enc_norm.weight\n",
      "name: value_encoder.enc_norm.bias\n",
      "freezing weights for: value_encoder.enc_norm.bias\n",
      "name: transformer_encoder.layers.0.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.0.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.weight\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.bias\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.weight\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.bias\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.weight\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.bias\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.weight\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.bias\n",
      "name: transformer_encoder.layers.1.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.1.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.weight\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.bias\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.weight\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.bias\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.weight\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.bias\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.weight\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.bias\n",
      "name: transformer_encoder.layers.2.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.2.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.weight\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.bias\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.weight\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.bias\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.weight\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.bias\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.weight\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.bias\n",
      "name: transformer_encoder.layers.3.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.3.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.weight\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.bias\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.weight\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.bias\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.weight\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.bias\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.weight\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.bias\n",
      "name: transformer_encoder.layers.4.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.4.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.weight\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.bias\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.weight\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.bias\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.weight\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.bias\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.weight\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.bias\n",
      "name: transformer_encoder.layers.5.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.5.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.weight\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.bias\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.weight\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.bias\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.weight\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.bias\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.weight\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.bias\n",
      "name: transformer_encoder.layers.6.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.6.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.weight\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.bias\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.weight\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.bias\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.weight\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.bias\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.weight\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.bias\n",
      "name: transformer_encoder.layers.7.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.7.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.weight\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.bias\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.weight\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.bias\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.weight\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.bias\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.weight\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.bias\n",
      "name: transformer_encoder.layers.8.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.8.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.weight\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.bias\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.weight\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.bias\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.weight\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.bias\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.weight\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.bias\n",
      "name: transformer_encoder.layers.9.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.9.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.weight\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.bias\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.weight\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.bias\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.weight\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.bias\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.weight\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.bias\n",
      "name: transformer_encoder.layers.10.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.10.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.weight\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.bias\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.weight\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.bias\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.weight\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.bias\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.weight\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.bias\n",
      "name: transformer_encoder.layers.11.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.weight\n",
      "name: transformer_encoder.layers.11.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.bias\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.weight\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.bias\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.weight\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.bias\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.weight\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.bias\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.weight\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.bias\n",
      "name: linear.weight\n",
      "name: linear.bias\n",
      "name: classified_decoder.fc1.weight\n",
      "name: classified_decoder.fc1.bias\n",
      "name: classified_decoder.bn1.weight\n",
      "name: classified_decoder.bn1.bias\n",
      "name: classified_decoder.fc2.weight\n",
      "name: classified_decoder.fc2.bias\n",
      "name: classified_decoder.bn2.weight\n",
      "name: classified_decoder.bn2.bias\n",
      "name: classified_decoder.fc3.weight\n",
      "name: classified_decoder.fc3.bias\n",
      "scGPT - INFO - Total Pre freeze Params 50726146\n",
      "scGPT - INFO - Total Post freeze Params 658690\n",
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): CategoryValueEncoder(\n",
      "    (embedding): Embedding(103, 512, padding_idx=101)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (classified_decoder): ClassificationDecoder(\n",
      "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation1): LeakyReLU(negative_slope=0.01)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation2): LeakyReLU(negative_slope=0.01)\n",
      "    (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load the model\n",
    "######################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=n_cls,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=False,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    num_batch_labels=False,\n",
    "    domain_spec_batchnorm=False,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    explicit_zero_prob=False,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading ALL model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        logger.info(f\"Loading SOME model params from {model_file}\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        # for k, v in pretrained_dict.items():\n",
    "        #     logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    # print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "\n",
    "    # if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(model)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace277c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Loss function\n",
    "######################################################################\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(), lr=lr, eps=1e-4)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma=0.9)\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62f20fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Train the model\n",
    "######################################################################\n",
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_num = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        gender = batch_data[\"gender\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=CLS,\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=False,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            loss = 0.0\n",
    "            loss = criterion_cls(output_dict[\"classified_output\"], gender)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * len(input_gene_ids)\n",
    "        total_num += len(input_gene_ids)\n",
    "\n",
    "        preds = output_dict[\"classified_output\"].argmax(dim=1).cpu().numpy()\n",
    "        labels = gender.cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    epoch_loss = total_loss / total_num\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "    logger.info(\n",
    "    f\"| train | total_num {total_num} | epoch loss {epoch_loss:5.4f} | precision {precision:5.4f} | \" \n",
    "    f\" recall {recall:5.4f} | f1 {f1:5.4f} |\" \n",
    "    f\" accuracy {accuracy:5.4f}\")\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train/loss\": epoch_loss,\n",
    "            \"train/precision\": precision,\n",
    "            \"train/recall\": recall,\n",
    "            \"train/f1\": f1,\n",
    "            \"train/accuracy\": accuracy,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11def3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Evaluate the model\n",
    "######################################################################\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            gender = batch_data[\"gender\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=False,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "\n",
    "                loss = criterion_cls(output_dict[\"classified_output\"], gender)\n",
    "            \n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "            \n",
    "            preds = output_dict[\"classified_output\"].argmax(dim=1).cpu().numpy()\n",
    "            labels = gender.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "\n",
    "    # 定义评估指标函数\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    val_loss = total_loss / total_num\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "     \n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/loss\": val_loss,\n",
    "            \"valid/precision\": precision,\n",
    "            \"valid/recall\": recall,\n",
    "            \"valid/f1\": f1,\n",
    "            \"valid/accuracy\": accuracy,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    logger.info(\n",
    "    f\"| valid | total_num {total_num} | epoch loss {val_loss:5.4f} | precision {precision:5.4f} | \" \n",
    "    f\" recall {recall:5.4f} | f1 {f1:5.4f} | \" \n",
    "    f\" accuracy {accuracy:5.4f}\")\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "\n",
    "    return val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "410999e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Apply the model on the age of the clusters \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e37c6000005da'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e37c7000005dc'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e21a8000005db'\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e3a8e000005df'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e3aa6000005de'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "Traceback (most recent call last):\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e3b08000005dd'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "Traceback (most recent call last):\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e38d5000005e1'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e387e000005e0'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e3a17000005e2'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e387b000005e3'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e38d2000005e5'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e3a90000005e6'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e38d9000005e7'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e3a22000005e4'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e38d4000005e8'\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004a7e3a21000005e9'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Epoch 1/2\n",
      "scGPT - INFO - | train | total_num 29843 | epoch loss 0.7431 | precision 0.5286 |  recall 0.4838 | f1 0.5052 | accuracy 0.5653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a12b000005f9'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa89762000005ee'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a233000005ea'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a12a000005f2'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa89d49000005f5'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa895d3000005f3'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a3bb000005f6'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a429000005f4'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa89760000005f8'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a127000005ed'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa899b8000005ec'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a61f000005eb'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a514000005f7'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a981000005ef'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a0ff000005f0'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8a42e000005f1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | valid | total_num 7461 | epoch loss 5.9518 | precision 0.4574 |  recall 1.0000 | f1 0.6277 |  accuracy 0.4574\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 5.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 734, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 690, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8f89300000609'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa9830900000602'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8f01c00000608'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8f787000005fe'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa9830b000005fc'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa9830700000601'\n",
      "  File \"/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/shutil.py\", line 688, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8e6a0000005fb'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8f08a00000606'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8f01b000005fa'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8f891000005fd'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8ffc800000605'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa980c300000607'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8e45000000603'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa98306000005ff'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8e6a200000600'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000004aa8f78500000604'\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 718.00 MiB. GPU 0 has a total capacty of 15.70 GiB of which 228.44 MiB is free. Process 1527895 has 7.06 GiB memory in use. Process 3543561 has 520.00 MiB memory in use. Including non-PyTorch memory, this process has 7.81 GiB memory in use. Of the allocated memory 6.09 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m\n\u001b[1;32m     20\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m prepare_dataloader(\n\u001b[1;32m     21\u001b[0m     valid_data_pt,\n\u001b[1;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39meval_batch_size,\n\u001b[1;32m     23\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(model,loader\u001b[38;5;241m=\u001b[39mvalid_loader)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# early stop\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m input_gene_ids\u001b[38;5;241m.\u001b[39meq(vocab[pad_token])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m---> 24\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCLS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCCE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMVC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mECS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#generative_training=False\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion_cls(output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassified_output\u001b[39m\u001b[38;5;124m\"\u001b[39m], gender)\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/project/code/sexPrediction/../sexPrediction/scgpt/model/model.py:416\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# transformer_output: (batch, seq_len, embsize)\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# if self.use_batch_labels:\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m#     batch_emb = self.batch_encoder(batch_labels)  # (batch, embsize)\u001b[39;00m\n\u001b[1;32m    423\u001b[0m output \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/project/code/sexPrediction/../sexPrediction/scgpt/model/model.py:248\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    245\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(total_embs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# total_embs: (batch, seq_len, d_model)   \u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/project/code/sexPrediction/../sexPrediction/scgpt/model/model.py:796\u001b[0m, in \u001b[0;36mFlashTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m     src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(src2)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 796\u001b[0m     src2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    797\u001b[0m     src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(src2)\n\u001b[1;32m    798\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(src)\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/flash_attn/flash_attention.py:99\u001b[0m, in \u001b[0;36mFlashMHA.forward\u001b[0;34m(self, x, key_padding_mask, need_weights)\u001b[0m\n\u001b[1;32m     97\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWqkv(x)\n\u001b[1;32m     98\u001b[0m qkv \u001b[38;5;241m=\u001b[39m rearrange(qkv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s (three h d) -> b s three h d\u001b[39m\u001b[38;5;124m'\u001b[39m, three\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[0;32m---> 99\u001b[0m context, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(rearrange(context, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s h d -> b s (h d)\u001b[39m\u001b[38;5;124m'\u001b[39m)), attn_weights\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/flash_attn/flash_attention.py:57\u001b[0m, in \u001b[0;36mFlashAttention.forward\u001b[0;34m(self, qkv, key_padding_mask, causal, cu_seqlens, max_s, need_weights)\u001b[0m\n\u001b[1;32m     55\u001b[0m         x_unpad, indices, cu_seqlens, max_s \u001b[38;5;241m=\u001b[39m unpad_input(x, key_padding_mask)\n\u001b[1;32m     56\u001b[0m         x_unpad \u001b[38;5;241m=\u001b[39m rearrange(x_unpad, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnz (three h d) -> nnz three h d\u001b[39m\u001b[38;5;124m'\u001b[39m, three\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39mnheads)\n\u001b[0;32m---> 57\u001b[0m         output_unpad \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attn_unpadded_qkvpacked_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx_unpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         output \u001b[38;5;241m=\u001b[39m rearrange(pad_input(rearrange(output_unpad, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnz h d -> nnz (h d)\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     62\u001b[0m                                     indices, batch_size, seqlen),\n\u001b[1;32m     63\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s (h d) -> b s h d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mnheads)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py:256\u001b[0m, in \u001b[0;36mflash_attn_unpadded_qkvpacked_func\u001b[0;34m(qkv, cu_seqlens, max_seqlen, dropout_p, softmax_scale, causal, return_attn_probs, deterministic)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflash_attn_unpadded_qkvpacked_func\u001b[39m(qkv, cu_seqlens, max_seqlen, dropout_p, softmax_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    232\u001b[0m                                        causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_attn_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dropout_p should be set to 0.0 during evaluation\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m        qkv: (total, 3, nheads, headdim), where total = total number of tokens in the batch.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            pattern (negative means that location was dropped, nonnegative means it was kept).\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlashAttnQKVPackedFunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attn_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py:58\u001b[0m, in \u001b[0;36mFlashAttnQKVPackedFunc.forward\u001b[0;34m(ctx, qkv, cu_seqlens, max_seqlen, dropout_p, softmax_scale, causal, return_softmax, deterministic)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m softmax_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     softmax_scale \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m out, softmax_lse, rng_state, S_dmask \u001b[38;5;241m=\u001b[39m \u001b[43m_flash_attn_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_softmax\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(qkv, out, softmax_lse, cu_seqlens, rng_state)\n\u001b[1;32m     64\u001b[0m ctx\u001b[38;5;241m.\u001b[39mdropout_p \u001b[38;5;241m=\u001b[39m dropout_p\n",
      "File \u001b[0;32m/vol/bitbucket/mr423/miniconda3/envs/scgpt/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py:21\u001b[0m, in \u001b[0;36m_flash_attn_forward\u001b[0;34m(q, k, v, out, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, return_softmax, num_splits, generator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flash_attn_forward\u001b[39m(q, k, v, out, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k,\n\u001b[1;32m     14\u001b[0m                         dropout_p, softmax_scale, causal, return_softmax, num_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     15\u001b[0m                         generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    num_splits: how much to parallelize over the seqlen_q dimension. num_splits=0 means\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    it will be set by an internal heuristic. We're exposing num_splits mostly for benchmarking.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Don't change it unless you know what you're doing.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     softmax_lse, rng_state, \u001b[38;5;241m*\u001b[39mrest \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attn_cuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# if out.isnan().any() or softmax_lse.isnan().any():\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#     breakpoint()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     S_dmask \u001b[38;5;241m=\u001b[39m rest[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m return_softmax \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 718.00 MiB. GPU 0 has a total capacty of 15.70 GiB of which 228.44 MiB is free. Process 1527895 has 7.06 GiB memory in use. Process 3543561 has 520.00 MiB memory in use. Including non-PyTorch memory, this process has 7.81 GiB memory in use. Of the allocated memory 6.09 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# 训练和验证循环\n",
    "######################################################################\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "\n",
    "logger.info(\"Apply the model on the age of the clusters \\n\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    train_data_pt, valid_data_pt = prepare_data()\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(model,loader=train_loader,)\n",
    "   \n",
    "    val_loss = evaluate(model,loader=valid_loader)\n",
    "\n",
    "    # early stop\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# save the model into the save_dir\n",
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bfa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Test the model\n",
    "######################################################################\n",
    "def test(model: nn.Module, adata: DataLoader):\n",
    "    \n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    # print(adata.layers[input_layer_key])\n",
    "\n",
    "    gender_labels = adata.obs[\"gender_id\"].tolist()\n",
    "    gender_labels = np.array(gender_labels)\n",
    "\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=False,\n",
    "    )\n",
    "\n",
    "    tensor_gender_test = torch.from_numpy(gender_labels).long()\n",
    "\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": tokenized_test[\"values\"],\n",
    "        \"gender\": tensor_gender_test,\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            gender = batch_data[\"gender\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=None,\n",
    "                    CLS=True, \n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=False,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                    \n",
    "                loss = criterion_cls(output_dict[\"classified_output\"], gender)\n",
    "            \n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "                   \n",
    "            preds = output_dict[\"classified_output\"].argmax(dim=1).cpu().numpy()\n",
    "            labels = gender.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "   \n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\"-\" * 89)\n",
    "    \n",
    "    logger.info(\n",
    "    f\"| test | total_num {total_num} | precision {precision:5.4f} | \" \n",
    "    f\" recall {recall:5.4f} | f1 {f1:5.4f} | \" \n",
    "    f\" accuracy {accuracy:5.4f}\")\n",
    "    logger.info(\"-\" * 89)    \n",
    "    \n",
    "    \n",
    "\n",
    "    # 保存数据到 CSV 文件\n",
    "    all_preds_list = all_preds.tolist()\n",
    "    all_labels_list = all_labels.tolist()\n",
    "\n",
    "    all_preds_save_dir = str(save_dir) + \"/all_preds.txt\"\n",
    "    all_labels_save_dir = str(save_dir) + \"/all_targets.txt\"\n",
    "\n",
    "    with open(all_preds_save_dir, \"w\") as file:\n",
    "        file.write(str(all_preds_list))\n",
    "\n",
    "    with open(all_labels_save_dir, \"w\") as file:\n",
    "        file.write(str(all_labels_list))\n",
    "\n",
    "    all_preds_df = pd.DataFrame(all_preds_list, columns=['Predictions'])\n",
    "    all_labels_df = pd.DataFrame(all_labels_list, columns=['Target'])\n",
    "\n",
    "    combined_df = pd.concat([all_preds_df, all_labels_df], axis=1)\n",
    "    combined_df['Predictions'] = combined_df['Predictions'].round(1)\n",
    "    combined_df['Target'] = combined_df['Target'].round(1)\n",
    "\n",
    "    combined_df.to_csv(str(save_dir) + \"/output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc792dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "test(model, adata_test)\n",
    "\n",
    "# %%\n",
    "\n",
    "df = pd.read_csv(str(save_dir) + \"/output.csv\")\n",
    "\n",
    "y_true = df['Target']\n",
    "y_pred = df['Predictions']\n",
    "\n",
    "\n",
    "# 定义评估指标函数\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1: {f1}')\n",
    "print(f'accuracy: {accuracy:.2f}')\n",
    "print(f'conf_matrix: {conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ecc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存 ROC 曲线和 PR 曲线图像\n",
    "# from sklearn.metrics import roc_auc_score, roc_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 计算 AUC-ROC\n",
    "# roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "# fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "\n",
    "# # 绘制并保存 ROC 曲线\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('roc_curve.png')  # 保存图片\n",
    "# plt.close()  # 关闭绘图窗口\n",
    "\n",
    "# print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "\n",
    "# from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# # 计算 AUC-PR\n",
    "# precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred_prob)\n",
    "# pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# # 绘制并保存 PR 曲线\n",
    "# plt.figure()\n",
    "# plt.plot(recall_vals, precision_vals, label=f'PR curve (area = {pr_auc:.2f})')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve')\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig('pr_curve.png')  # 保存图片\n",
    "# plt.close()  # 关闭绘图窗口\n",
    "\n",
    "# print(f\"AUC-PR: {pr_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521338d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
