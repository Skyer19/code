{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated temp directory: /data/mr423/tmp\n",
      "['/data/mr423/project/code/sexPrediction', '/data/mr423/miniconda3-2d/envs/scgpt/lib/python39.zip', '/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9', '/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/lib-dynload', '', '/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages']\n",
      "scgpt location:  /data/mr423/project/code/sexPrediction/scgpt/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'do_train': True, 'load_model': '/data/mr423/project/pre_trained_model/scGPT_human', 'n_bins': 101, 'epochs': 1, 'lr': 0.0001, 'batch_size': 128, 'layer_size': 512, 'nlayers': 12, 'nhead': 8, 'dropout': 0.0, 'use_fast_transformer': True, 'pre_norm': False, 'amp': True, 'freeze': True}\n",
      "save to /data/mr423/project/code/sexPrediction/record/dev_biobank-gender_pred-Sep07-22-15\n",
      "(37304, 2919)\n",
      "(4145, 2919)\n",
      "{0: 0, 1: 1}\n",
      "scGPT - INFO - match 2895/2919 genes in vocabulary of size 60697.\n",
      "**** actual model parameters ****\n",
      "layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** actual model parameters ****\n",
      "\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - train set number of samples: 29843, \n",
      "\t feature length: 2896\n",
      "scGPT - INFO - valid set number of samples: 7461, \n",
      "\t feature length: 2896\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 检查 tempfile 模块使用的临时文件目录\n",
    "temp_dir = tempfile.gettempdir()\n",
    "print(\"Updated temp directory:\", temp_dir)\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# new_path = '../sexPrediction/'\n",
    "# if new_path not in sys.path:\n",
    "#     sys.path.insert(0, new_path)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "# relaod the scgpt files\n",
    "import scgpt\n",
    "print(\"scgpt location: \", scgpt.__file__)\n",
    "importlib.reload(scgpt)\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"WANDB_MODE\"]= \"offline\"\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for wandb mentior\n",
    "######################################################################\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    do_train=True,\n",
    "    load_model=\"/data/mr423/project/pre_trained_model/scGPT_human\",\n",
    "    n_bins=101,\n",
    "\n",
    "    epochs=1, # 2 !!!!!!!!!!!!  test only\n",
    "    lr=0.0001,\n",
    "    batch_size=128,   # 128 !!!!!!!!!!!!  test only\n",
    "\n",
    "    layer_size=512, # 128\n",
    "    nlayers=12,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=8,  # number of heads in nn.MultiheadAttention\n",
    "    \n",
    "    dropout=0.0,  # dropout probability\n",
    "\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    freeze = True, #freeze\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"gender_pred\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for input and preprocessing\n",
    "######################################################################\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "input_emb_style = \"category\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "\n",
    "\n",
    "# settings for training\n",
    "CLS = True  # celltype classification objective\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for optimizer\n",
    "######################################################################\n",
    "lr = config.lr\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "early_stop = 10\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for the model\n",
    "######################################################################\n",
    "use_fast_transformer = config.use_fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "\n",
    "\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Validate the settings\n",
    "######################################################################\n",
    "# assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "# assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Settings for the running recording\n",
    "######################################################################\n",
    "dataset_name = 'biobank-gender_pred'\n",
    "save_dir = Path(f\"/data/mr423/project/code/sexPrediction/record/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Data loading\n",
    "######################################################################\n",
    "adata = sc.read(\"/data/mr423/project/data/3-OLINK_data_train_withOutlier_all.h5ad\")\n",
    "adata_test = sc.read(\"/data/mr423/project/data/3-OLINK_data_test_withOutlier_all.h5ad\")\n",
    "\n",
    "print(adata.shape)\n",
    "print(adata_test.shape)\n",
    "\n",
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\" \n",
    "\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "\n",
    "data_is_raw = False\n",
    "\n",
    "adata_test_raw = adata_test.copy()\n",
    "adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "\n",
    "gender_id_labels = adata.obs[\"sex\"].astype(\"category\").cat.codes.values\n",
    "# ageGroup_types = adata.obs[\"Age_Group\"].unique()\n",
    "adata.obs[\"gender_id\"] = gender_id_labels\n",
    "\n",
    "\n",
    "n_cls = len(np.unique(gender_id_labels))\n",
    "\n",
    "id2type = dict(enumerate(adata.obs[\"sex\"].astype(\"category\").cat.categories))\n",
    "print(id2type)\n",
    "\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The pre-trained model\n",
    "######################################################################\n",
    "if config.load_model is not None:\n",
    "    model_dir = config.load_model\n",
    "    # model_config_file = model_dir + \"/args.json\"\n",
    "    # model_file = model_dir + \"/best_model.pt\"\n",
    "    # vocab_file = model_dir + \"/vocab.json\"\n",
    "\n",
    "    model_file = \"/data/mr423/project/code/sexPrediction/record/dev_biobank-gender_pred-Sep05-19-53/model.pt\"\n",
    "    vocab_file = \"/data/mr423/project/code/sexPrediction/record/dev_biobank-gender_pred-Sep05-19-53/vocab.json\"    \n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # # model\n",
    "    # with open(model_config_file, \"r\") as f:\n",
    "    #     model_configs = json.load(f)\n",
    "    # logger.info(\n",
    "    #     f\"Resume model from {model_file}, the model args will override the \"\n",
    "    #     f\"config {model_config_file}.\"\n",
    "    # )\n",
    "    # embsize = model_configs[\"embsize\"]\n",
    "    # nhead = model_configs[\"nheads\"]\n",
    "    # d_hid = model_configs[\"d_hid\"]\n",
    "    # nlayers = model_configs[\"nlayers\"]\n",
    "    # n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "    # print(\"\\n**** parameters from the pre-trained model ****\")\n",
    "    # print(f'layer_size = embsize: {model_configs[\"embsize\"]} = d_hid: {model_configs[\"d_hid\"]}, n_layers: {model_configs[\"nlayers\"]}, nhead: {model_configs[\"nheads\"]}')\n",
    "    # print(\"**** parameters from the pre-trained model ****\\n\")\n",
    "\n",
    "    print(\"**** actual model parameters ****\")\n",
    "    print(f'layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "    print(\"**** actual model parameters ****\\n\")\n",
    "\n",
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=False,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=3000,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Split the data to train and test\n",
    "######################################################################\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "gender_labels = adata.obs[\"gender_id\"].tolist()  # make sure count from 0\n",
    "gender_labels = np.array(gender_labels)\n",
    "\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_gender,\n",
    "    valid_gender,\n",
    ") = train_test_split(\n",
    "    all_counts, gender_labels, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Tokenize the data\n",
    "######################################################################\n",
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "\n",
    "    input_values_train, input_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_gender_train = torch.from_numpy(train_gender).long()\n",
    "    tensor_gender_valid = torch.from_numpy(valid_gender).long()\n",
    "\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"gender\": tensor_gender_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"gender\": tensor_gender_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " **** load model parameters ****\n",
      "lr = 0.0001, batch_size = 128, epochs = 1\n",
      "ntokens = 60697, layer_size = embsize: 512 = d_hid: 512, n_layers: 12, nhead: 8\n",
      "**** load model parameters ****\n",
      "\n",
      "scGPT - INFO - Loading ALL model params from /data/mr423/project/code/sexPrediction/record/dev_biobank-gender_pred-Sep05-19-53/model.pt\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Load the model\n",
    "######################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "\n",
    "print(\"\\n\\n **** load model parameters ****\")\n",
    "print(f'lr = {lr}, batch_size = {batch_size}, epochs = {epochs}')\n",
    "print(f'ntokens = {ntokens}, layer_size = embsize: {embsize} = d_hid: {d_hid}, n_layers: {nlayers}, nhead: {nhead}')\n",
    "print(\"**** load model parameters ****\\n\")\n",
    "\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=n_cls,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=False,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    num_batch_labels=False,\n",
    "    domain_spec_batchnorm=False,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    explicit_zero_prob=False,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading ALL model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        logger.info(f\"Loading SOME model params from {model_file}\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        # for k, v in pretrained_dict.items():\n",
    "        #     logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "# print(model)\n",
    "wandb.watch(model)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Loss function\n",
    "######################################################################\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(), lr=lr, eps=1e-4)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma=0.9)\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = (\n",
    "    adata_test.layers[input_layer_key].A\n",
    "    if issparse(adata_test.layers[input_layer_key])\n",
    "    else adata_test.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "# print(adata.layers[input_layer_key])\n",
    "\n",
    "gender_labels = adata_test.obs[\"gender_id\"].tolist()\n",
    "gender_labels = np.array(gender_labels)\n",
    "\n",
    "\n",
    "tokenized_test = tokenize_and_pad_batch(\n",
    "    all_counts,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=False,\n",
    ")\n",
    "\n",
    "tensor_gender_test = torch.from_numpy(gender_labels).long()\n",
    "\n",
    "\n",
    "test_data_pt = {\n",
    "    \"gene_ids\": tokenized_test[\"genes\"],\n",
    "    \"values\": tokenized_test[\"values\"],\n",
    "    \"gender\": tensor_gender_test,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=SeqDataset(test_data_pt),\n",
    "    batch_size=eval_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | test | total_num 4145 | precision 0.9948 |  recall 0.9948 | f1 0.9948 |  accuracy 0.9952\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test(model: nn.Module):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            gender = batch_data[\"gender\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=None,\n",
    "                    CLS=True, \n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=False,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                    \n",
    "                loss = criterion_cls(output_dict[\"classified_output\"], gender)\n",
    "            \n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "                   \n",
    "            preds = output_dict[\"classified_output\"].argmax(dim=1).cpu().numpy()\n",
    "            labels = gender.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "   \n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\"-\" * 89)\n",
    "    \n",
    "    logger.info(\n",
    "    f\"| test | total_num {total_num} | precision {precision:5.4f} | \" \n",
    "    f\" recall {recall:5.4f} | f1 {f1:5.4f} | \" \n",
    "    f\" accuracy {accuracy:5.4f}\")\n",
    "    logger.info(\"-\" * 89) \n",
    "\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 反向传播，计算梯度\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# 收集每个样本的梯度\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m         gradients\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnoisy_input_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 平均多个样本的梯度\u001b[39;00m\n\u001b[1;32m     49\u001b[0m avg_gradients \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(gradients, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cpu'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for batch_data in test_loader:\n",
    "    input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "    input_values = batch_data[\"values\"].to(device)\n",
    "    gender = batch_data[\"gender\"].to(device).long()\n",
    "    \n",
    "    model.eval()\n",
    "    gradients = []\n",
    "\n",
    "\n",
    "    src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "    noise_level = 0.1\n",
    "    num_samples = 50\n",
    "    for _ in range(num_samples):\n",
    "        # 添加噪声并确保 `requires_grad=True`，以便在反向传播时计算梯度\n",
    "        noisy_input_values = input_values + noise_level * torch.randn_like(input_values)\n",
    "        noisy_input_values.requires_grad = True  # 确保启用梯度追踪\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                noisy_input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=True, \n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=False,\n",
    "                #generative_training = False,\n",
    "            )\n",
    "            # 计算损失并进行反向传播\n",
    "\n",
    "            # print(f'output_dict[\"classified_output\"] {output_dict[\"classified_output\"]}')\n",
    "            output_values = output_dict[\"classified_output\"]\n",
    "            loss = criterion_cls(output_values, gender)\n",
    "\n",
    "            # loss = F.binary_cross_entropy_with_logits(output_values, gender.float())\n",
    "\n",
    "            model.zero_grad()  # 清除之前的梯度\n",
    "            loss.backward()  # 反向传播，计算梯度\n",
    "\n",
    "            # 收集每个样本的梯度\n",
    "            gradients.append(noisy_input_values.grad.cpu().numpy())\n",
    "\n",
    "    # 平均多个样本的梯度\n",
    "    avg_gradients = np.mean(gradients, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    # 可视化平均梯度值，显示哪些基因对预测最重要\n",
    "    gene_names = adata.var[\"gene_name\"].tolist()\n",
    "    important_genes = np.mean(np.abs(avg_gradients), axis=0)  # 对不同样本取平均值\n",
    "    \n",
    "    sorted_indices = np.argsort(-important_genes)  # 由大到小排序\n",
    "    top_genes = [gene_names[i] for i in sorted_indices[:20]]  # 获取最重要的前20个基因\n",
    "    top_importances = important_genes[sorted_indices[:20]]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_importances, y=top_genes)\n",
    "    plt.xlabel(\"Not importance Score\")\n",
    "    plt.ylabel(\"Plasma protein-encoding genes\")\n",
    "    # plt.title(\"Top 20 not important Genes for Age Prediction\")\n",
    "\n",
    "    output_path = \"./top_no_important_genes_smoothgrad.png\"  # 你可以修改保存路径和文件名\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")  # dpi 300确保高质量保存，bbox_inches=\"tight\" 去除空白边框\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    break  # 只处理一个批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
