{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9406b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/scanpy/_settings.py:488: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b5d67",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for cell-type annotation task\n",
    "Listed below are some hyper-parameter recommendations for the cell-type task. Note that the CLS objective is on to facilitate cell-type classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07b5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    # dataset_name=\"ms\",\n",
    "    do_train=True,\n",
    "    load_model=\"/data/mr423/pre_trained_model/scGPT_blood\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=100,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-3,\n",
    "    batch_size=128,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.5,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = True, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6a3467-1059-4eaa-89a1-eda393a0c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#     'method': 'grid', #grid, random\n",
    "#     'metric': {\n",
    "#       'name': 'accuracy',\n",
    "#       'goal': 'maximize'   \n",
    "#     },\n",
    "#     'parameters': {\n",
    "#         'dropout': {\n",
    "#             'values': [0.2, 0.4, 0.5]\n",
    "#         }, \n",
    "#         'lr': {\n",
    "#             'values': [1e-2, 1e-3, 1e-4]\n",
    "#         },\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c08ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstr1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/mr423/project/code/wandb/run-20240807_172929-9u1b030z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/str1/age_prediction-test/runs/9u1b030z' target=\"_blank\">solar-star-24</a></strong> to <a href='https://wandb.ai/str1/age_prediction-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/str1/age_prediction-test' target=\"_blank\">https://wandb.ai/str1/age_prediction-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/str1/age_prediction-test/runs/9u1b030z' target=\"_blank\">https://wandb.ai/str1/age_prediction-test/runs/9u1b030z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'do_train': True, 'load_model': '/data/mr423/pre_trained_model/scGPT_blood', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.001, 'batch_size': 128, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.5, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': True, 'DSBN': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"age_prediction-test\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7890b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ff2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7112d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_biobank-Aug07-17-29\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'biobank'\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc7002",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data\n",
    "We follow the standard scGPT data pre-processing pipelines for the cell-type annotation task. Note that since now we have two datasets at hand (i.e., reference and query data), the same pre-prpocessing steps need to be applied to both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758a09aa-356c-4005-b5ae-ba5bbe84dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"../data/3-OLINK_data_sub_train.h5ad\")\n",
    "adata_test = sc.read(\"../data/3-OLINK_data_sub_test.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991da1b7-c2ab-41e2-a45c-24a564d41751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5803, 2919)\n",
      "(2487, 2919)\n"
     ]
    }
   ],
   "source": [
    "print(adata.shape)\n",
    "print(adata_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8cf724-d675-410c-8fe7-88635783cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2985182-c4ac-416a-9b4d-a8ebac340bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963ff317-4dab-4b01-8882-747b1422fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "adata_test_raw = adata_test.copy()\n",
    "adata = adata.concatenate(adata_test, batch_key=\"str_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee74293-0b59-4f7d-ae18-c52ec08616d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>DoB_Year</th>\n",
       "      <th>DoB_Month</th>\n",
       "      <th>DoB_Day</th>\n",
       "      <th>DoB</th>\n",
       "      <th>Date_Attend</th>\n",
       "      <th>age</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>str_batch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4345870-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1954</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1954-05-15</td>\n",
       "      <td>2006-05-09</td>\n",
       "      <td>51.983573</td>\n",
       "      <td>50-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4470636-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1962</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1962-04-15</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>45.779603</td>\n",
       "      <td>40-50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3236504-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1946</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1946-10-15</td>\n",
       "      <td>2008-05-31</td>\n",
       "      <td>61.626283</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306041-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1943</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1943-05-15</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>66.880219</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306375-0</th>\n",
       "      <td>0</td>\n",
       "      <td>1944</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1944-03-15</td>\n",
       "      <td>2007-11-15</td>\n",
       "      <td>63.668720</td>\n",
       "      <td>60-70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex  DoB_Year  DoB_Month  DoB_Day         DoB Date_Attend  \\\n",
       "Id                                                                     \n",
       "4345870-0    0      1954          5       15  1954-05-15  2006-05-09   \n",
       "4470636-0    1      1962          4       15  1962-04-15  2008-01-25   \n",
       "3236504-0    0      1946         10       15  1946-10-15  2008-05-31   \n",
       "3306041-0    1      1943          5       15  1943-05-15  2010-04-01   \n",
       "3306375-0    0      1944          3       15  1944-03-15  2007-11-15   \n",
       "\n",
       "                 age Age_Group batch_id str_batch  \n",
       "Id                                                 \n",
       "4345870-0  51.983573     50-60        0         0  \n",
       "4470636-0  45.779603     40-50        0         0  \n",
       "3236504-0  61.626283     60-70        0         0  \n",
       "3306041-0  66.880219     60-70        0         0  \n",
       "3306375-0  63.668720     60-70        0         0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6fde36b-a083-495c-8052-3610974527dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "# celltype_id_labels = adata.obs[\"Age_Group\"].astype(\"category\").cat.codes.values\n",
    "# celltypes = adata.obs[\"Age_Group\"].unique()\n",
    "\n",
    "\n",
    "num_types = 1\n",
    "# num_types = len(np.unique(celltype_id_labels))\n",
    "# id2type = dict(enumerate(adata.obs[\"Age_Group\"].astype(\"category\").cat.categories))\n",
    "\n",
    "print(num_types)\n",
    "\n",
    "# adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0966573-8287-4ad6-a176-9ef4b34d3c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>DoB_Year</th>\n",
       "      <th>DoB_Month</th>\n",
       "      <th>DoB_Day</th>\n",
       "      <th>DoB</th>\n",
       "      <th>Date_Attend</th>\n",
       "      <th>age</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>str_batch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1275089-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1940-05-15</td>\n",
       "      <td>2009-11-27</td>\n",
       "      <td>69.535934</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436569-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1948</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>1948-09-15</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>61.467488</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986938-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1942</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1942-08-15</td>\n",
       "      <td>2009-09-15</td>\n",
       "      <td>67.085558</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405742-1</th>\n",
       "      <td>1</td>\n",
       "      <td>1946</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1946-07-15</td>\n",
       "      <td>2009-03-24</td>\n",
       "      <td>62.691307</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801719-1</th>\n",
       "      <td>0</td>\n",
       "      <td>1953</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1953-08-15</td>\n",
       "      <td>2007-07-20</td>\n",
       "      <td>53.927447</td>\n",
       "      <td>50-60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex  DoB_Year  DoB_Month  DoB_Day         DoB Date_Attend  \\\n",
       "Id                                                                     \n",
       "1275089-1    1      1940          5       15  1940-05-15  2009-11-27   \n",
       "1436569-1    0      1948          9       15  1948-09-15  2010-03-05   \n",
       "4986938-1    0      1942          8       15  1942-08-15  2009-09-15   \n",
       "2405742-1    1      1946          7       15  1946-07-15  2009-03-24   \n",
       "2801719-1    0      1953          8       15  1953-08-15  2007-07-20   \n",
       "\n",
       "                 age Age_Group  batch_id str_batch  \n",
       "Id                                                  \n",
       "1275089-1  69.535934     60-70         1         1  \n",
       "1436569-1  61.467488     60-70         1         1  \n",
       "4986938-1  67.085558     60-70         1         1  \n",
       "2405742-1  62.691307     60-70         1         1  \n",
       "2801719-1  53.927447     50-60         1         1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6de4ee-1d48-45d3-8163-1ef8ed07005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_group_counts = adata.obs['celltype_id'].value_counts().sort_index()\n",
    "# age_group_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20428ef2-6db4-4540-9152-3f482702b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_group_counts = adata.obs['Age_Group'].value_counts().sort_index()\n",
    "# age_group_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dc5a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2889/2919 genes in vocabulary of size 36574.\n",
      "scGPT - INFO - Resume model from /data/mr423/pre_trained_model/scGPT_blood/best_model.pt, the model args will override the config /data/mr423/pre_trained_model/scGPT_blood/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = config.load_model\n",
    "    model_config_file = model_dir + \"/args.json\"\n",
    "    model_file = model_dir + \"/best_model.pt\"\n",
    "    vocab_file = model_dir + \"/vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4edd885-dd69-4734-931c-3ba0bc6a1de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d08757ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18c0569c-53da-4399-b371-50bdc16449b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex               int64\n",
       "DoB_Year          int64\n",
       "DoB_Month         int64\n",
       "DoB_Day           int64\n",
       "DoB            category\n",
       "Date_Attend    category\n",
       "age             float64\n",
       "Age_Group      category\n",
       "batch_id           int8\n",
       "str_batch      category\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93aa6138-77f6-4104-9f42-21ed03c00c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex               int64\n",
       "DoB_Year          int64\n",
       "DoB_Month         int64\n",
       "DoB_Day           int64\n",
       "DoB            category\n",
       "Date_Attend    category\n",
       "age             float64\n",
       "Age_Group      category\n",
       "batch_id           int8\n",
       "str_batch      category\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_test.obs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc1b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.9835729  45.77960301 61.62628337 ... 64.11225188 58.15742642\n",
      " 51.73442847]\n"
     ]
    }
   ],
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "age = adata.obs[\"age\"].tolist()\n",
    "age = np.array(age)\n",
    "\n",
    "print(age)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_age,\n",
    "    valid_age,\n",
    ") = train_test_split(\n",
    "    all_counts, age, test_size=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84f8c123-bb31-4125-8fed-43169046fdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5222, 2889)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8019eb2-9c8b-4797-8d50-f512d45af8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48.11498973, 60.64065708, 59.34291581, ..., 50.76249144,\n",
       "       46.50513347, 52.42710472])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4cedad2-f6f6-487d-93a8-854e042aad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valiaad_age = np.array([24, 28, 32, 36, 42, 48, 52, 58, 62, 68, 74, 78, 82, 88, 94, 98, 102, 108, 114, 118])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea3f98be-7a37-4016-95b2-67fc3ed7b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valiaad_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cd701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "818bfcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 5222, \n",
      "\t feature length: 2890\n",
      "scGPT - INFO - valid set number of samples: 581, \n",
      "\t feature length: 2890\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4920a117-a6d5-4b7d-94ea-6997e8b29f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genes': tensor([[36572,  8381,  8385,  ..., 36571, 36571, 36571],\n",
       "         [36572,  8381,  8385,  ..., 36571, 36571, 36571],\n",
       "         [36572,  8381,  8385,  ..., 12884, 35421, 36571],\n",
       "         ...,\n",
       "         [36572,  8381,  8385,  ..., 35421, 36571, 36571],\n",
       "         [36572,  8381,  8385,  ..., 35421, 36571, 36571],\n",
       "         [36572,  8381,  8385,  ...,  4939, 12884, 35421]]),\n",
       " 'values': tensor([[ 0., 28., 37.,  ..., -2., -2., -2.],\n",
       "         [ 0., 20., 34.,  ..., -2., -2., -2.],\n",
       "         [ 0.,  6., 13.,  ...,  2., 34., -2.],\n",
       "         ...,\n",
       "         [ 0.,  5.,  9.,  ..., 19., -2., -2.],\n",
       "         [ 0., 13., 12.,  ..., 26., -2., -2.],\n",
       "         [ 0., 13., 43.,  ..., 21.,  1., 23.]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e33206d-31b5-42ee-aeba-55a498f97453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37a80818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    # masked_values_train = random_mask_value(\n",
    "    #     tokenized_train[\"values\"],\n",
    "    #     mask_ratio=mask_ratio,\n",
    "    #     mask_value=mask_value,\n",
    "    #     pad_value=pad_value,\n",
    "    # )\n",
    "    # masked_values_valid = random_mask_value(\n",
    "    #     tokenized_valid[\"values\"],\n",
    "    #     mask_ratio=mask_ratio,\n",
    "    #     mask_value=mask_value,\n",
    "    #     pad_value=pad_value,\n",
    "    # )\n",
    "    # print(\n",
    "    #     f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "    #     f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    # )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    \n",
    "    # input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    \n",
    "    input_values_train, input_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    # tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    # tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    # tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    # tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    \n",
    "\n",
    "    tensor_age_train = torch.from_numpy(train_age).float()\n",
    "    tensor_age_valid = torch.from_numpy(valid_age).float()\n",
    "\n",
    "    # print(\"prepare_data function : -- tensor_age_train\",tensor_age_train)\n",
    "\n",
    "    # scaler = MinMaxScaler()\n",
    "    # train_age_normalized = scaler.fit_transform(train_age.reshape(-1, 1))\n",
    "    # valid_age_normalized = scaler.transform(valid_age.reshape(-1, 1))\n",
    "\n",
    "    # print(\"prepare_data function : -- train_age_normalized\",train_age_normalized)\n",
    "\n",
    "    # tensor_age_train = torch.from_numpy(train_age_normalized).float()\n",
    "    # tensor_age_valid = torch.from_numpy(valid_age_normalized).float()\n",
    "\n",
    "    # print(\"prepare_data function : -- tensor_age_train\",tensor_age_train)\n",
    "\n",
    "\n",
    "    \n",
    "    # if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "    #     train_sort_ids = np.argsort(train_batch_labels)\n",
    "    #     input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "    #     input_values_train = input_values_train[train_sort_ids]\n",
    "    #     target_values_train = target_values_train[train_sort_ids]\n",
    "    #     tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "    #     tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "\n",
    "    #     valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "    #     input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "    #     input_values_valid = input_values_valid[valid_sort_ids]\n",
    "    #     target_values_valid = target_values_valid[valid_sort_ids]\n",
    "    #     # tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "    #     tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"age\": tensor_age_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"age\": tensor_age_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8add613c-294f-49bb-954e-381c967c37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    # if per_seq_batch_sample:\n",
    "    #     # find the indices of samples in each seq batch\n",
    "    #     subsets = []\n",
    "    #     batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "    #     for batch_label in np.unique(batch_labels_array):\n",
    "    #         batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "    #         subsets.append(batch_indices)\n",
    "    #     data_loader = DataLoader(\n",
    "    #         dataset=dataset,\n",
    "    #         batch_sampler=SubsetsBatchSampler(\n",
    "    #             subsets,\n",
    "    #             batch_size,\n",
    "    #             intra_subset_shuffle=intra_domain_shuffle,\n",
    "    #             inter_subset_shuffle=shuffle,\n",
    "    #             drop_last=drop_last,\n",
    "    #         ),\n",
    "    #         num_workers=num_workers,\n",
    "    #         pin_memory=True,\n",
    "    #     )\n",
    "    #     return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77105fda",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "219bb9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([36574, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "freezing weights for: encoder.embedding.weight\n",
      "freezing weights for: encoder.enc_norm.weight\n",
      "freezing weights for: encoder.enc_norm.bias\n",
      "freezing weights for: value_encoder.linear1.weight\n",
      "freezing weights for: value_encoder.linear1.bias\n",
      "freezing weights for: value_encoder.linear2.weight\n",
      "freezing weights for: value_encoder.linear2.bias\n",
      "freezing weights for: value_encoder.norm.weight\n",
      "freezing weights for: value_encoder.norm.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.0.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.1.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.2.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.3.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.4.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.5.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.6.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.7.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.8.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.9.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.10.norm2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.Wqkv.bias\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "freezing weights for: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.linear2.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm1.bias\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.weight\n",
      "freezing weights for: transformer_encoder.layers.11.norm2.bias\n",
      "scGPT - INFO - Total Pre freeze Params 38981122\n",
      "scGPT - INFO - Total Post freeze Params 1053698\n",
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(36574, 512, padding_idx=36571)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): ContinuousValueEncoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x FlashTransformerEncoderLayer(\n",
      "        (self_attn): FlashMHA(\n",
      "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (inner_attn): FlashAttention()\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ExprDecoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cls_decoder): ClsDecoder(\n",
      "    (_decoder): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (sim): Similarity(\n",
      "    (cos): CosineSimilarity()\n",
      "  )\n",
      "  (creterion_cce): CrossEntropyLoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    # print(\"-\"*20)\n",
    "    # print(f\"name: {name}\")\n",
    "    # if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "wandb.watch(model)\n",
    "\n",
    "# if ADV:\n",
    "#     discriminator = AdversarialDiscriminator(\n",
    "#         d_model=embsize,\n",
    "#         n_cls=num_batch_types,\n",
    "#     ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bc5c1fe-eafa-4e10-a794-b0bb28cac2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NonNegativeMSELoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NonNegativeMSELoss, self).__init__()\n",
    "#         self.mse = nn.MSELoss()\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "#         y_pred = torch.clamp(y_pred, min=0, max = 1)\n",
    "#         return self.mse(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e4ea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion_cls = nn.MSELoss()\n",
    "# criterion_cls = NonNegativeMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06ffa2c6-c2c3-4b3f-866b-9f5041610919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_sigmoid(predictions):\n",
    "    # sigmoid = torch.nn.Sigmoid()\n",
    "    # return sigmoid(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d15923a-3ee9-4a50-a399-b9c02664f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_num = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        \n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        age = batch_data[\"age\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "            \n",
    "            loss = 0.0\n",
    "            metrics_to_log = {}\n",
    "                \n",
    "            # loss = criterion_cls(apply_sigmoid(output_dict[\"cls_output\"]), age)\n",
    "            output_values = output_dict[\"cls_output\"]\n",
    "            loss = criterion_cls(output_values, age)\n",
    "\n",
    "            print(\"output : \",output_values)\n",
    "            print(\"ground : \",age)\n",
    "            \n",
    "            # print(\"output after apply_sigmoid: \",apply_sigmoid(output_dict[\"cls_output\"]))\n",
    "\n",
    "                  \n",
    "            # print(\"train total_loss: \",loss)\n",
    "           \n",
    "            # metrics_to_log.update({\"train/cls\": loss.item()})\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # wandb.log(metrics_to_log)\n",
    "        # total_loss += loss.item()\n",
    "        total_loss += loss.item() * len(input_gene_ids)\n",
    "        total_num += len(input_gene_ids)\n",
    "        \n",
    "     \n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "                        \n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    " \n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {optimizer.param_groups[0]['lr']:05.4f} | ms/batch {ms_per_batch:5.4f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \")\n",
    "            \n",
    "            start_time = time.time()\n",
    "    epoch_loss = total_loss / total_num\n",
    "    # logger.info(f\"Epoch {epoch}/{epochs}, Epoch Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch}/{epochs}, Epoch Loss: {epoch_loss:.4f}\")   \n",
    "    metrics_to_log.update({\"train/loss\": epoch_loss})\n",
    "    wandb.log(metrics_to_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28212ef5-3593-425b-8329-61284c7c2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估指标函数\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred)).item()\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2).item()\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred) ** 2)).item()\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    y_mean = torch.mean(y_true)\n",
    "    total_variance = torch.sum((y_true - y_mean) ** 2)\n",
    "    residual_variance = torch.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (residual_variance / total_variance).item()\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true)).item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21de17f2-41af-4875-a48f-165e22ce83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mae\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/rmse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/r2\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mape\", summary=\"min\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9101fc2b-fd1c-48be-ba85-19a124d52f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            age = batch_data[\"age\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                \n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, age)\n",
    "\n",
    "                # print(\"evaluate loss: \",loss)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)     \n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "            \n",
    "            # 保存预测值和真实值以计算其他评估指标\n",
    "            all_preds.append(output_values.cpu())\n",
    "            all_targets.append(age.cpu())\n",
    "\n",
    "    # 将所有批次的预测值和真实值连接起来\n",
    "    all_preds = torch.cat(all_preds)\n",
    "\n",
    "    # print('all_preds :', all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    # print('all_targets :', all_targets)\n",
    "    \n",
    "    # 计算其他评估指标\n",
    "    total_mae = mean_absolute_error(all_targets, all_preds)\n",
    "    total_rmse = root_mean_squared_error(all_targets, all_preds)\n",
    "    total_r2 = r_squared(all_targets, all_preds)\n",
    "    total_mape = mean_absolute_percentage_error(all_targets, all_preds)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mae\": total_mae,\n",
    "            \"valid/rmse\": total_rmse,\n",
    "            \"valid/r2\": total_r2,\n",
    "            \"valid/mape\": total_mape,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return total_loss / total_num, total_mae, total_rmse, total_r2, total_mape\n",
    "\n",
    "\n",
    "    # wandb.log(\n",
    "    #     {\n",
    "    #         \"valid/mse\": total_loss / total_num,\n",
    "    #         \"epoch\": epoch,\n",
    "    #     },\n",
    "    # )\n",
    "\n",
    "\n",
    "    # return total_loss / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54ec1aa0-7cb2-4fc3-bd4f-e20a2df25324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Apply the model on the age of the prediction \n",
      "\n",
      "output :  tensor([[57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ground :  tensor([60.6105, 63.5702, 58.7023, 60.5530, 46.4613, 64.9391, 61.4949, 59.2553,\n",
      "        52.5667, 60.9336, 67.8303, 56.8378, 54.8309, 41.8042, 57.0815, 63.5921,\n",
      "        47.9425, 47.5154, 49.5414, 60.3833, 55.8275, 59.7837, 57.1800, 49.5797,\n",
      "        70.1821, 54.9049, 45.9521, 62.4860, 60.4244, 51.2909, 66.0999, 41.8645,\n",
      "        70.1602, 63.1102, 61.7467, 64.6407, 51.8905, 44.4244, 61.3361, 64.9035,\n",
      "        60.8953, 52.0821, 62.7570, 54.8665, 61.1882, 46.1547, 59.3840, 64.0000,\n",
      "        64.5750, 53.9138, 64.3477, 44.6598, 57.9740, 69.5031, 51.2115, 66.8802,\n",
      "        52.4408, 49.9740, 57.0541, 65.7303, 69.8371, 44.2382, 61.0842, 61.5332,\n",
      "        58.7461, 57.2457, 64.0712, 54.9952, 47.7481, 53.2813, 58.7461, 49.4593,\n",
      "        47.4497, 50.3436, 41.0240, 61.0267, 46.8939, 62.5407, 64.2656, 65.6208,\n",
      "        62.2313, 46.2642, 52.2409, 56.6872, 65.7166, 48.8843, 65.9767, 52.8049,\n",
      "        43.8385, 61.4292, 64.9199, 56.4353, 46.7187, 62.4367, 62.3628, 57.4538,\n",
      "        66.2533, 49.1389, 64.2464, 54.9076, 63.0034, 68.0192, 57.1663, 48.3532,\n",
      "        46.6721, 67.7864, 63.3703, 64.9007, 61.7385, 51.3429, 53.5058, 55.8960,\n",
      "        68.7556, 60.9035, 66.5489, 40.4298, 50.0260, 65.3060, 41.5113, 59.0883,\n",
      "        69.9001, 60.5996, 64.2984, 67.2444, 66.2368, 61.7550, 69.4237, 65.6701],\n",
      "       device='cuda:0')\n",
      "output :  tensor([[57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.3750],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ground :  tensor([56.5503, 54.7953, 50.7488, 64.9856, 62.1191, 69.7084, 51.1704, 54.8118,\n",
      "        46.2149, 47.2498, 61.1554, 41.8809, 67.1759, 56.8323, 68.3559, 66.2752,\n",
      "        61.2841, 55.5948, 68.3915, 61.9877, 64.3066, 63.9890, 47.3949, 40.9582,\n",
      "        59.4825, 45.9357, 57.6181, 68.1068, 46.3463, 69.0869, 52.4682, 64.8433,\n",
      "        66.3491, 60.0164, 62.9569, 48.6982, 61.9302, 55.0144, 66.5270, 69.5277,\n",
      "        63.4497, 63.6906, 48.5147, 68.2081, 65.6181, 63.8686, 50.9760, 67.1978,\n",
      "        41.6345, 59.9288, 44.2902, 59.6961, 48.4572, 59.1677, 59.3018, 61.3963,\n",
      "        63.8987, 57.6318, 68.1232, 66.2122, 50.2286, 59.0746, 60.7912, 58.8036,\n",
      "        66.5270, 60.9418, 57.1198, 66.0123, 66.1766, 53.8891, 50.8309, 65.0787,\n",
      "        49.6756, 53.0924, 56.9144, 66.0835, 62.4422, 65.9384, 54.6037, 41.2758,\n",
      "        66.6776, 46.5106, 62.8419, 59.7426, 57.4292, 43.4251, 44.0794, 49.6619,\n",
      "        69.3990, 63.0335, 62.3326, 56.6790, 58.6092, 55.7810, 47.7618, 64.2464,\n",
      "        45.4593, 49.5907, 48.5257, 61.0815, 65.5852, 60.8378, 65.3990, 65.9685,\n",
      "        67.2663, 66.9350, 68.5147, 67.1732, 62.0862, 64.3587, 44.8925, 68.5038,\n",
      "        53.1006, 58.7242, 57.6619, 64.6543, 67.1102, 64.6626, 41.2183, 49.5907,\n",
      "        69.0623, 67.0856, 49.6016, 63.9042, 54.7680, 54.3080, 52.0411, 41.0595],\n",
      "       device='cuda:0')\n",
      "output :  tensor([[57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ground :  tensor([64.7611, 61.9493, 64.3012, 47.2909, 68.9144, 52.5996, 65.6044, 51.4962,\n",
      "        65.6893, 66.9870, 62.8775, 69.2402, 63.0144, 67.4524, 61.8070, 46.4778,\n",
      "        69.3169, 53.5414, 67.6961, 52.9719, 69.3114, 51.2170, 65.5825, 47.5099,\n",
      "        59.4114, 52.3504, 63.1650, 57.1253, 64.9829, 49.9055, 68.4928, 56.4189,\n",
      "        63.3977, 62.5325, 59.6742, 62.2642, 63.8220, 60.4709, 62.3655, 60.9309,\n",
      "        53.3279, 60.1862, 56.8734, 57.2320, 69.9493, 49.8179, 45.2211, 56.1561,\n",
      "        47.1266, 59.9973, 69.8371, 64.2136, 43.7016, 66.3354, 48.4600, 52.0465,\n",
      "        69.1116, 52.4134, 56.6680, 69.9357, 69.6674, 52.2683, 58.5243, 63.9589,\n",
      "        50.9295, 58.9240, 65.2758, 44.2875, 54.7899, 65.8919, 56.2245, 63.9288,\n",
      "        67.4031, 67.4935, 54.1273, 69.8508, 64.4189, 43.5428, 45.4894, 63.9014,\n",
      "        62.7762, 62.1027, 62.4339, 67.3429, 41.8782, 64.5147, 55.0253, 42.0178,\n",
      "        55.3347, 41.2183, 60.6872, 45.0869, 49.9001, 54.6694, 66.0890, 46.0233,\n",
      "        69.2840, 61.3251, 64.2793, 44.8816, 55.9343, 63.8275, 59.2936, 61.7796,\n",
      "        50.1574, 59.2663, 58.7953, 55.0910, 68.0548, 61.5250, 59.5866, 50.1492,\n",
      "        53.4264, 51.0308, 69.7632, 63.5209, 47.1567, 62.8036, 55.5510, 41.9630,\n",
      "        66.9268, 47.4168, 43.5373, 52.8186, 62.8118, 66.7680, 51.1951, 65.5880],\n",
      "       device='cuda:0')\n",
      "output :  tensor([[57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062],\n",
      "        [57.4062],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ground :  tensor([50.2834, 57.9411, 58.2533, 63.5784, 62.7652, 45.2293, 69.9877, 52.9035,\n",
      "        65.1855, 64.5640, 46.3107, 52.9254, 65.8891, 59.4114, 51.0034, 62.3737,\n",
      "        65.2019, 49.7029, 59.8330, 66.1328, 69.5907, 68.0630, 56.3970, 58.7817,\n",
      "        54.7296, 55.8522, 63.4524, 45.3443, 55.4716, 62.9240, 60.8296, 67.7755,\n",
      "        63.5483, 50.4559, 62.6585, 59.6468, 62.9322, 68.0903, 64.6105, 52.5038,\n",
      "        53.6290, 61.7769, 49.0897, 63.8385, 62.4668, 42.2615, 43.0527, 64.5695,\n",
      "        61.0979, 62.7023, 53.8754, 62.1109, 56.5886, 57.2841, 59.6030, 54.4997,\n",
      "        55.9370, 59.3648, 55.6496, 48.3313, 54.5681, 65.4209, 59.9808, 64.9281,\n",
      "        66.5681, 58.7543, 61.7440, 62.2505, 64.1999, 61.8289, 56.5229, 62.6831,\n",
      "        68.4381, 50.2669, 61.3963, 63.9699, 53.2676, 42.1191, 59.5318, 44.7529,\n",
      "        66.7187, 64.3285, 45.0459, 61.3415, 47.6523, 63.2580, 57.8179, 54.9979,\n",
      "        62.4504, 49.1882, 57.0815, 48.7228, 62.5599, 59.3949, 41.7303, 66.8501,\n",
      "        60.4956, 41.4949, 44.3669, 60.5038, 62.6667, 56.8843, 62.4723, 61.7029,\n",
      "        45.2156, 48.3477, 55.8987, 62.0507, 46.6968, 59.4031, 64.1971, 55.8029,\n",
      "        63.0637, 50.5982, 70.1191, 48.9336, 51.4168, 53.1636, 56.4736, 64.6571,\n",
      "        61.3087, 42.3190, 53.9001, 65.5660, 67.7454, 44.3012, 48.7885, 40.5777],\n",
      "       device='cuda:0')\n",
      "output :  tensor([[57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ground :  tensor([49.0322, 65.7632, 69.9028, 54.5161, 60.5832, 54.5407, 51.2444, 54.3956,\n",
      "        59.3101, 59.6386, 54.9185, 54.0397, 61.1964, 52.4463, 67.2033, 52.8679,\n",
      "        68.9035, 59.2142, 61.3799, 45.9329, 56.4244, 69.2293, 60.6680, 51.5784,\n",
      "        55.1157, 60.7447, 51.1869, 50.4093, 59.6194, 62.9432, 42.8118, 66.4695,\n",
      "        42.7981, 59.4031, 41.1280, 53.4209, 48.3395, 44.8296, 55.4524, 54.4476,\n",
      "        54.3573, 41.3552, 67.9288, 65.8070, 67.8111, 61.9521, 53.5113, 57.6235,\n",
      "        52.6571, 61.5140, 52.3833, 59.2526, 46.2231, 56.1424, 46.8583, 63.6769,\n",
      "        60.4709, 57.5168, 43.7618, 44.8843, 63.1376, 69.7303, 61.1034, 60.4189,\n",
      "        54.2067, 64.6982, 53.1061, 46.4641, 61.6810, 63.7372, 70.0014, 69.7659,\n",
      "        61.1691, 55.9973, 55.3840, 60.9829, 67.3511, 45.0568, 43.1841, 47.8987,\n",
      "        60.8022, 67.8850, 50.8364, 42.8145, 61.3799, 43.0664, 62.3737, 58.8747,\n",
      "        58.5544, 70.0479, 64.2683, 62.3929, 66.8775, 67.3292, 57.3744, 46.6886,\n",
      "        54.5161, 46.2834, 64.3039, 58.3901, 59.8795, 53.4565, 64.1807, 63.0582,\n",
      "        41.2841, 62.1958, 42.2259, 40.2491, 46.0205, 57.2512, 55.4196, 65.2813,\n",
      "        43.3018, 67.1978, 66.8036, 65.0705, 53.8836, 66.2396, 62.9788, 60.6188,\n",
      "        46.4613, 54.9076, 57.7331, 64.9692, 47.9617, 59.1513, 67.9288, 53.0349],\n",
      "       device='cuda:0')\n",
      "output :  tensor([[57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4688],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375],\n",
      "        [57.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ground :  tensor([62.8775, 65.3443, 64.4572, 66.4613, 47.4962, 69.2621, 58.3984, 68.4654,\n",
      "        67.6112, 62.7762, 55.3676, 63.2142, 57.9028, 68.6845, 54.2012, 42.4668,\n",
      "        65.5797, 60.1013, 60.7310, 64.4216, 66.9158, 63.2334, 56.0027, 52.5229,\n",
      "        64.9281, 54.5927, 69.2019, 53.5962, 64.9719, 70.0397, 41.9274, 47.0418,\n",
      "        69.3032, 65.7769, 50.9569, 48.8433, 68.7064, 68.3778, 65.6619, 52.7447,\n",
      "        61.4264, 50.1711, 61.9357, 62.3436, 47.8166, 66.9322, 59.9589, 55.0554,\n",
      "        65.6290, 62.3847, 59.4415, 46.0808, 66.3107, 65.8864, 52.1697, 54.6502,\n",
      "        54.4066, 64.0329, 64.9391, 63.1732, 59.1321, 43.5811, 53.6345, 61.8480,\n",
      "        55.5838, 44.6461, 44.4134, 60.5092, 63.4141, 57.3662, 57.7851, 52.2875,\n",
      "        62.2779, 62.0014, 63.5428, 42.4832, 50.6010, 63.7454, 66.5489, 53.2019,\n",
      "        66.2478, 61.2238, 69.2786, 59.5181, 41.3881, 46.0780, 63.6030, 57.5195,\n",
      "        56.8186, 57.2101, 57.7906, 53.3333, 48.1670, 64.1150, 56.3751, 68.8624,\n",
      "        56.3313, 62.2177, 47.4004, 63.6112, 70.1410, 56.3422, 61.7960, 52.3723,\n",
      "        56.2875, 55.4470, 47.0527, 48.3450, 53.9083, 66.0424, 68.4901, 60.7639,\n",
      "        70.3655, 57.9658, 65.3936, 67.6797, 60.4709, 50.1766, 48.8788, 53.6263,\n",
      "        40.8405, 67.3183, 56.7036, 51.3402, 65.2129, 48.1780, 62.2560, 54.5544],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m prepare_dataloader(\n\u001b[1;32m     21\u001b[0m     valid_data_pt,\n\u001b[1;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39meval_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# val_loss = evaluate(model,loader=valid_loader)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m val_loss, total_mae, total_rmse, total_r2, total_mape \u001b[38;5;241m=\u001b[39m evaluate(model,loader\u001b[38;5;241m=\u001b[39mvalid_loader)\n",
      "Cell \u001b[0;32mIn[42], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m---> 24\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mINPUT_BATCH_LABELS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDSBN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCLS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCCE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMVC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMVC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mECS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample_in_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#generative_training=False\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     38\u001b[0m     metrics_to_log \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/scgpt/model/model.py:194\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(total_embs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/mr423/miniconda3-2d/envs/scgpt/lib/python3.9/site-packages/scgpt/model/model.py:696\u001b[0m, in \u001b[0;36mFlashTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlashTransformerEncoderLayer does not support src_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# no padding tokens in src\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     src_key_padding_mask_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "\n",
    "logger.info(\"Apply the model on the age of the prediction \\n\")\n",
    "\n",
    "for epoch in range(1, 10 + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    if config.do_train:\n",
    "        train(model,loader=train_loader,)\n",
    "\n",
    "    # val_loss = evaluate(model,loader=valid_loader)\n",
    "\n",
    "    val_loss, total_mae, total_rmse, total_r2, total_mape = evaluate(model,loader=valid_loader)\n",
    "    \n",
    "    # scheduler.step(val_loss)  # 更新学习率调度器\n",
    "\n",
    "    \n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    \n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "    f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | mse {val_loss:5.4f} | \" \n",
    "    f\" mae {total_mae:5.4f} | rmse {total_rmse:5.4f} | \" \n",
    "    f\" r2 {total_r2:5.4f} | mape {total_mape:5.4f}\")\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "    \n",
    "\n",
    "# save the model into the save_dir\n",
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd7105-df8d-4c14-ab46-97ca38dfc19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361200e-657c-4ffd-9ccd-b2dffb14c15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8334be-21d6-4a61-8f9c-1dd76d43e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844f543-87fe-4ef5-8702-b7671231714e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab5837-049c-4980-bba9-9fd0b675c2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ad2c4-fd6b-4b7b-b5f9-35cf33a26df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698771e-c601-4b7b-aae7-0490caeb5f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a3c83-282a-44fc-974f-f3a5f8bcba42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258c614-4815-4ffa-8833-914153ffcaef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "#     \"\"\"\n",
    "#     Train the model for one epoch.\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     (\n",
    "#         total_loss,\n",
    "#         total_mse,\n",
    "#         total_cls,\n",
    "#         total_cce,\n",
    "#         total_mvc,\n",
    "#         total_ecs,\n",
    "#         total_dab,\n",
    "#         total_adv_E,\n",
    "#         total_adv_D,\n",
    "#         total_zero_log_prob,\n",
    "#         total_mvc_zero_log_prob,\n",
    "#     ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "#     total_error = 0.0\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     num_batches = len(loader)\n",
    "#     for batch, batch_data in enumerate(loader):\n",
    "#         input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "#         input_values = batch_data[\"values\"].to(device)\n",
    "#         # target_values = batch_data[\"target_values\"].to(device)\n",
    "#         # batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "#         # celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "        \n",
    "#         age = batch_data[\"age\"].to(device)\n",
    "\n",
    "#         src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "#         with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "#             output_dict = model(\n",
    "#                 input_gene_ids,\n",
    "#                 input_values,\n",
    "#                 src_key_padding_mask=src_key_padding_mask,\n",
    "#                 batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "#                 CLS=CLS,\n",
    "#                 CCE=CCE,\n",
    "#                 MVC=MVC,\n",
    "#                 ECS=ECS,\n",
    "#                 do_sample=do_sample_in_train,\n",
    "#                 #generative_training=False\n",
    "#             )\n",
    "\n",
    "#             # masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "#             loss = 0.0\n",
    "#             metrics_to_log = {}\n",
    "#             # if MLM:\n",
    "#             #     loss_mse = criterion(\n",
    "#             #         output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "#             #     )\n",
    "#             #     loss = loss + loss_mse\n",
    "#             #     metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "#             # if explicit_zero_prob:\n",
    "#             #     loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "#             #         output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "#             #     )\n",
    "#             #     loss = loss + loss_zero_log_prob\n",
    "#             #     metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "#             if CLS:\n",
    "#                 # print(\"output: \",output_dict[\"cls_output\"])\n",
    "#                 # print(\"ground: \",age)\n",
    "                \n",
    "#                 loss_cls = criterion_cls(output_dict[\"cls_output\"], age)\n",
    "#                 loss = loss + loss_cls\n",
    "#                 metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "#                 # error_rate = 1 - (\n",
    "#                 #     (output_dict[\"cls_output\"].argmax(1) == age)\n",
    "#                 #     .sum()\n",
    "#                 #     .item()\n",
    "#                 # ) / celltype_labels.size(0)\n",
    "#             # if CCE:\n",
    "#             #     loss_cce = 10 * output_dict[\"loss_cce\"]\n",
    "#             #     loss = loss + loss_cce\n",
    "#             #     metrics_to_log.update({\"train/cce\": loss_cce.item()})\n",
    "#             # if MVC:\n",
    "#             #     loss_mvc = criterion(\n",
    "#             #         output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "#             #     )\n",
    "#             #     loss = loss + loss_mvc\n",
    "#             #     metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n",
    "#             # if MVC and explicit_zero_prob:\n",
    "#             #     loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "#             #         output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "#             #     )\n",
    "#             #     loss = loss + loss_mvc_zero_log_prob\n",
    "#             #     metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n",
    "#             # if ECS:\n",
    "#             #     loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "#             #     loss = loss + loss_ecs\n",
    "#             #     metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "#             # if DAB:\n",
    "#             #     # try weighting and separate optimizer\n",
    "#             #     loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "#             #     loss = loss + dab_weight * loss_dab\n",
    "#             #     metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "#         model.zero_grad()\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.unscale_(optimizer)\n",
    "#         with warnings.catch_warnings(record=True) as w:\n",
    "#             warnings.filterwarnings(\"always\")\n",
    "#             torch.nn.utils.clip_grad_norm_(\n",
    "#                 model.parameters(),\n",
    "#                 1.0,\n",
    "#                 error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "#             )\n",
    "#             if len(w) > 0:\n",
    "#                 logger.warning(\n",
    "#                     f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "#                     f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "#                     \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "#                 )\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         # if ADV:\n",
    "#         #     # rerun the model for adversarial training\n",
    "#         #     output_dict = model(\n",
    "#         #         input_gene_ids,\n",
    "#         #         input_values,\n",
    "#         #         src_key_padding_mask=src_key_padding_mask,\n",
    "#         #         batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "#         #         CLS=CLS,\n",
    "#         #         CCE=CCE,\n",
    "#         #         MVC=MVC,\n",
    "#         #         ECS=ECS,\n",
    "#         #         do_sample=do_sample_in_train,\n",
    "#         #         #generative_training=False\n",
    "#         #     )\n",
    "\n",
    "#         #     # TRAINING DISCRIMINATOR\n",
    "#         #     loss_adv_D = criterion_adv(\n",
    "#         #         discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n",
    "#         #     )\n",
    "#         #     if epoch > adv_D_delay_epochs:\n",
    "#         #         discriminator.zero_grad()\n",
    "#         #         loss_adv_D.backward()\n",
    "#         #         optimizer_D.step()\n",
    "\n",
    "#         #     # TRAINING ENCODER\n",
    "#         #     loss_adv_E = -criterion_adv(\n",
    "#         #         discriminator(output_dict[\"cell_emb\"]), batch_labels\n",
    "#         #     )\n",
    "#         #     # NOTE: the loss is negative here because we want to maximize\n",
    "#         #     # the cross_entropy_loss, in other words, disguise against the discriminator\n",
    "#         #     if epoch > adv_E_delay_epochs:\n",
    "#         #         model.zero_grad()\n",
    "#         #         discriminator.zero_grad()\n",
    "#         #         loss_adv_E.backward()\n",
    "#         #         optimizer_E.step()\n",
    "\n",
    "#         wandb.log(metrics_to_log)\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         total_mse += loss_mse.item() if MLM else 0.0\n",
    "#         total_cls += loss_cls.item() if CLS else 0.0\n",
    "#         total_cce += loss_cce.item() if CCE else 0.0\n",
    "#         total_mvc += loss_mvc.item() if MVC else 0.0\n",
    "#         total_ecs += loss_ecs.item() if ECS else 0.0\n",
    "#         total_dab += loss_dab.item() if DAB else 0.0\n",
    "#         total_adv_E += loss_adv_E.item() if ADV else 0.0\n",
    "#         total_adv_D += loss_adv_D.item() if ADV else 0.0\n",
    "#         total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n",
    "#         total_mvc_zero_log_prob += (\n",
    "#             loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n",
    "#         )\n",
    "#         # total_error += error_rate\n",
    "#         if batch % log_interval == 0 and batch > 0:\n",
    "            \n",
    "#             # lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "#             ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "#             cur_loss = total_loss / log_interval\n",
    "#             cur_mse = total_mse / log_interval\n",
    "#             cur_cls = total_cls / log_interval if CLS else 0.0\n",
    "#             cur_cce = total_cce / log_interval if CCE else 0.0\n",
    "#             cur_mvc = total_mvc / log_interval if MVC else 0.0\n",
    "#             cur_ecs = total_ecs / log_interval if ECS else 0.0\n",
    "#             cur_dab = total_dab / log_interval if DAB else 0.0\n",
    "#             cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n",
    "#             cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n",
    "#             cur_zero_log_prob = (\n",
    "#                 total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n",
    "#             )\n",
    "#             cur_mvc_zero_log_prob = (\n",
    "#                 total_mvc_zero_log_prob / log_interval\n",
    "#                 if MVC and explicit_zero_prob\n",
    "#                 else 0.0\n",
    "#             )\n",
    "#             cur_error = total_error / log_interval\n",
    "#             # ppl = math.exp(cur_loss)\n",
    "#             logger.info(\n",
    "#                 f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "#                 f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "#                 f\"loss {cur_loss:5.2f} | \"\n",
    "#                 + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n",
    "#                 + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "#                 + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "#                 + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n",
    "#                 + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n",
    "#                 + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n",
    "#                 + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n",
    "#                 + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n",
    "#                 + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n",
    "#                 + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n",
    "#                 + (\n",
    "#                     f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n",
    "#                     if MVC and explicit_zero_prob\n",
    "#                     else \"\"\n",
    "#                 )\n",
    "#             )\n",
    "#             total_loss = 0\n",
    "#             total_mse = 0\n",
    "#             total_cls = 0\n",
    "#             total_cce = 0\n",
    "#             total_mvc = 0\n",
    "#             total_ecs = 0\n",
    "#             total_dab = 0\n",
    "#             total_adv_E = 0\n",
    "#             total_adv_D = 0\n",
    "#             total_zero_log_prob = 0\n",
    "#             total_mvc_zero_log_prob = 0\n",
    "#             total_error = 0\n",
    "#             start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c5051-341b-4922-a2dc-cf6059a54890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_wandb_metrcis():\n",
    "#     wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"test/avg_bio\", summary=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed8ca3-cecf-4ea6-8169-971b90c36a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "#     \"\"\"\n",
    "#     Evaluate the model on the evaluation data.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     total_error = 0.0\n",
    "#     total_dab = 0.0\n",
    "#     total_num = 0\n",
    "#     predictions = []\n",
    "\n",
    "#     total_truth = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch_data in loader:\n",
    "#             input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "#             input_values = batch_data[\"values\"].to(device)\n",
    "#             target_values = batch_data[\"target_values\"].to(device)\n",
    "#             # batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "#             # celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "#             age = batch_data[\"age\"].to(device)\n",
    "\n",
    "#             src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "#             with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "#                 output_dict = model(\n",
    "#                     input_gene_ids,\n",
    "#                     input_values,\n",
    "#                     src_key_padding_mask=src_key_padding_mask,\n",
    "#                     batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "#                     CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "#                     CCE=False,\n",
    "#                     MVC=False,\n",
    "#                     ECS=False,\n",
    "#                     do_sample=do_sample_in_train,\n",
    "#                     #generative_training = False,\n",
    "#                 )\n",
    "#                 output_values = output_dict[\"cls_output\"]\n",
    "#                 loss = criterion_cls(output_values, age)\n",
    "\n",
    "#                 # if DAB:\n",
    "#                 #     loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "#             total_loss += loss.item() * len(input_gene_ids)\n",
    "#             # accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "\n",
    "#             # total_truth +=accuracy\n",
    "#             # total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "#             total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            \n",
    "#             total_num += len(input_gene_ids)\n",
    "            \n",
    "#             # preds = output_values.argmax(1).cpu().numpy()\n",
    "#             # predictions.append(preds)\n",
    "\n",
    "#     wandb.log(\n",
    "#         {\n",
    "#             \"valid/mse\": total_loss / total_num,\n",
    "#             # \"valid/err\": total_error / total_num,\n",
    "#             # \"accuracy\": total_truth/total_num,\n",
    "#             # \"valid/sum_mse_dab\": (total_loss + dab_weight * total_dab) / total_num,\n",
    "#             \"epoch\": epoch,\n",
    "#         },\n",
    "#     )\n",
    "\n",
    "#     # if return_raw:\n",
    "#     #     return np.concatenate(predictions, axis=0)\n",
    "\n",
    "#     return total_loss / total_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a503c",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_loss = float(\"inf\")\n",
    "# best_avg_bio = 0.0\n",
    "# best_model = None\n",
    "# define_wandb_metrcis()\n",
    "\n",
    "# # run = wandb.init()\n",
    "\n",
    "# logger.info(\"Apply the model on the age of the clusters \\n\")\n",
    "\n",
    "# for epoch in range(1, 10 + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "\n",
    "#     # print(train_data_pt)\n",
    "\n",
    "\n",
    "\n",
    "#     train_loader = prepare_dataloader(\n",
    "#         train_data_pt,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         intra_domain_shuffle=True,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "\n",
    "#     # valid_loader = prepare_dataloader(\n",
    "#     #     valid_data_pt,\n",
    "#     #     batch_size=eval_batch_size,\n",
    "#     #     shuffle=False,\n",
    "#     #     intra_domain_shuffle=False,\n",
    "#     #     drop_last=False,\n",
    "#     # )\n",
    "\n",
    "\n",
    "#     if config.do_train:\n",
    "#         train(\n",
    "#             model,\n",
    "#             loader=train_loader,\n",
    "#         )\n",
    "\n",
    "\n",
    "#     val_loss = evaluate(\n",
    "#         model,\n",
    "#         loader=valid_loader,\n",
    "#     )\n",
    "#     elapsed = time.time() - epoch_start_time\n",
    "#     logger.info(\"-\" * 89)\n",
    "#     logger.info(\n",
    "#         f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "#         f\"valid loss/mse {val_loss:5.4f}\"\n",
    "#     )\n",
    "#     logger.info(\"-\" * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model = copy.deepcopy(model)\n",
    "#         best_model_epoch = epoch\n",
    "#         logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "#     scheduler.step()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     # if DAB_separate_optim:\n",
    "#     #     scheduler_dab.step()\n",
    "#     # if ADV:\n",
    "#     #     scheduler_D.step()\n",
    "#     #     scheduler_E.step()\n",
    "        \n",
    "# # wandb.agent(sweep_id, train)\n",
    "# # save the model into the save_dir\n",
    "# torch.save(best_model.state_dict(), save_dir / \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bfdfc-66d8-474e-9609-0ac717c75696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ce176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% inference\n",
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16236bf2",
   "metadata": {},
   "source": [
    "## Step 5: Inference with fine-tuned scGPT model\n",
    "In the cell-type annotation task, the fine-tuned scGPT predicts cell-type labels for query set as inference. The model performance is evaluated on standard classificaton metrics. Here we visualize the predicted labels over the scGPT cell embeddings, and present the confusion matrix for detailed classification performance on the cell-group level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79730e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]\n",
    "\n",
    "print(adata_test_raw.obs.head())\n",
    "\n",
    "\n",
    "print(len(np.unique(adata_test_raw.obs[\"predictions\"])))\n",
    "\n",
    "# # plot\n",
    "# palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] \n",
    "# palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "# palette_ = {c: palette_[i] for i, c in enumerate(celltypes)}\n",
    "\n",
    "# with plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n",
    "#     sc.pl.umap(\n",
    "#         adata_test_raw,\n",
    "#         color=[\"celltype\", \"predictions\"],\n",
    "#         palette=palette_,\n",
    "#         show=False,\n",
    "#     )\n",
    "#     plt.savefig(save_dir / \"results.png\", dpi=300)\n",
    "\n",
    "# save_dict = {\n",
    "#     \"predictions\": predictions,\n",
    "#     \"labels\": labels,\n",
    "#     \"results\": results,\n",
    "#     \"id_maps\": id2type\n",
    "# }\n",
    "# with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(save_dict, f)\n",
    "\n",
    "# results[\"test/cell_umap\"] = wandb.Image(\n",
    "#     str(save_dir / \"results.png\"),\n",
    "#     caption=f\"predictions macro f1 {results['test/macro_f1']:.3f}\",\n",
    "# )\n",
    "# wandb.log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "celltypes = list(celltypes)\n",
    "for i in set([id2type[p] for p in predictions]):\n",
    "    if i not in celltypes:\n",
    "        celltypes.remove(i)\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, index=celltypes[:cm.shape[0]], columns=celltypes[:cm.shape[1]])\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"Blues\")\n",
    "plt.savefig(save_dir / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "results[\"test/confusion_matrix\"] = wandb.Image(\n",
    "    str(save_dir / \"confusion_matrix.png\"),\n",
    "    caption=f\"confusion matrix\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model into the save_dir\n",
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4dbcf-1acc-4f38-a876-b10d7cff57cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d7f35-9506-4dba-b1d6-4fa2b671d036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6b3a7-d44c-4ef5-9fb2-bda8e5371f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b39928-f1ae-485b-9661-de39cbf5c81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9729b-80f4-479d-9f91-9394684df242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631767a-60f5-4e1b-820b-ec13b48b8e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scgpt)",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
