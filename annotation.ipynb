{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b1cd111c-b18d-4931-9145-b09d32ea18ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /data/mr423/miniconda3-2d/envs/scgpt/bin/python\n",
      "Python version: 3.9.19 (main, May  6 2024, 19:43:03) \n",
      "[GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 打印 Python 解释器的路径\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "# 打印 Python 版本\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e33845fb-9948-4472-81e3-1bb43e23f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated temp directory: /data/mr423/tmp\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# new_temp_dir = \"/data/mr423/tmp\"\n",
    "# os.makedirs(new_temp_dir, exist_ok=True)\n",
    "# os.environ[\"TMPDIR\"] = new_temp_dir  # 对于 Unix/Linux\n",
    "# os.environ[\"TMP\"] = new_temp_dir  # 适用于所有平台\n",
    "\n",
    "# 检查 tempfile 模块使用的临时文件目录\n",
    "temp_dir = tempfile.gettempdir()\n",
    "print(\"Updated temp directory:\", temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "50fc6580-9265-4b18-a446-10884842ce6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scgpt' from '/data/mr423/scGPT/scgpt/__init__.py'>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "new_path = '/data/mr423/scGPT/'\n",
    "\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "import importlib\n",
    "import scgpt\n",
    "importlib.reload(scgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "304162b6-ca38-4e9f-9b5d-8a109e3fdb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mr423/scGPT/scgpt/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(scgpt.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "68e0e5a3-fd61-49b5-9998-4a2e8d176096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# sys.path.insert(0, \"../\")\n",
    "\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6c1357e0-f33d-4011-8fa8-9cf02a86f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"protein\",\n",
    "    do_train=True,\n",
    "    load_model=\"../pre_trained_model/scGPT_blood\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=30,\n",
    "    n_bins=101,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-3,\n",
    "    batch_size=32,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.5,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = True,\n",
    "    freeze = True, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c3ca09c7-717d-42c6-b851-26767401c3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:29y5tidy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>error_rate</td><td>▅▇▆▅▆▂▃▁▁▅▃▇▅▁▂▅▃█▆▄▃▆▅▅▃▅▇█▄▅▅▄▃▄▅▆▆▇▇▅</td></tr><tr><td>info/post_freeze_param_count</td><td>▁</td></tr><tr><td>info/pre_freeze_param_count</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▂▄▂▄▄▃▅▄▄▂▂▄▃▃▃▂▁▂▄▂▃▂▃▃▂▃▂▂▂▄▄▃▃▂▄▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>error_rate</td><td>0.1875</td></tr><tr><td>info/post_freeze_param_count</td><td>19729417</td></tr><tr><td>info/pre_freeze_param_count</td><td>38510089</td></tr><tr><td>train_loss</td><td>1.95334</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ann1</strong> at: <a href='https://wandb.ai/str1/scGPT/runs/29y5tidy' target=\"_blank\">https://wandb.ai/str1/scGPT/runs/29y5tidy</a><br/> View project at: <a href='https://wandb.ai/str1/scGPT' target=\"_blank\">https://wandb.ai/str1/scGPT</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240725_114200-29y5tidy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:29y5tidy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d22fd3ddc784c00864bf2987b514181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113052333368817, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/mr423/project/wandb/run-20240725_120533-jyj7fv1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/str1/scGPT/runs/jyj7fv1x' target=\"_blank\">ann1</a></strong> to <a href='https://wandb.ai/str1/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/str1/scGPT' target=\"_blank\">https://wandb.ai/str1/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/str1/scGPT/runs/jyj7fv1x' target=\"_blank\">https://wandb.ai/str1/scGPT/runs/jyj7fv1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'dataset_name': 'protein', 'do_train': True, 'load_model': '../pre_trained_model/scGPT_blood', 'mask_ratio': 0.0, 'epochs': 30, 'n_bins': 101, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.001, 'batch_size': 32, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.5, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': True, 'DSBN': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    name=\"ann1\",\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "16983230-ce21-40df-9f22-0ac79b417f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explicit_zero_prob:  False\n"
     ]
    }
   ],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "'''\n",
    "input_style 决定了模型输入数据的预处理和表示方式。常见的选项包括：\n",
    "\"normed_raw\"：归一化的原始数据\n",
    "\"log1p\"：对数据进行 log(1 + x) 转换\n",
    "\"binned\"：将数据分箱处理\n",
    "'''\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "\n",
    "'''\n",
    "MLM (Masked Language Modeling) 是一种预训练目标，广泛用于像 BERT 这样的 transformer 模型中。\n",
    "它通过随机遮蔽输入文本中的某些 token，然后要求模型预测这些被遮蔽的 token，从而学习上下文信息\n",
    "\n",
    "CLS:(Celltype Classification Objective) 是指模型在训练过程中包含一个细胞类型分类的目标。这通常应用于生物信息学领域，尤其是在单细胞 RNA 测序数据分析中。\n",
    "\n",
    "ADV (Adversarial Training for Batch Correction) 是一种训练策略，用于纠正批次效应。\n",
    "批次效应是指在不同实验批次中数据分布的变化，这在生物信息学数据处理中是一个常见问题。\n",
    "\n",
    "CCE (Contrastive Cell Embedding Objective) 是一种训练目标，通过对比学习（contrastive learning）来优化细胞嵌入表示。\n",
    "如果 config.ecs_thres 大于 0，则启用此目标。\n",
    "对比学习通过将相似的样本拉近、不同的样本推远来学习数据的结构。\n",
    "\n",
    "ECS (Elastic Cell Similarity) 是一种训练目标，旨在优化细胞嵌入的相似性。通过设置一个阈值，模型会尝试使相似的细胞在嵌入空间中更接近。\n",
    "如果 config.ecs_thres 大于 0，则启用此目标。\n",
    "\n",
    "DAB (Domain Adaptation by Reverse Backpropagation) 是一种对抗训练策略，用于跨域适应。通过反向传播调整模型参数，使模型能够在不同域间更好地泛化。\n",
    "设置为 False，表示不使用此策略。如果设置为 2，则使用单独的优化器进行训练。\n",
    "\n",
    "INPUT_BATCH_LABELS: 控制是否使用输入的批次标签。这些标签可以帮助 MLM 和 MVC 目标，但不用于分类器\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "input_emb_style 参数决定了输入数据在模型中的嵌入方式。根据不同的数据类型和任务需求，选择合适的嵌入方式可以显著影响模型的性能。\n",
    "\n",
    "Category (\"category\")\n",
    "解释：选择 \"category\" 作为嵌入风格时，输入数据被认为是类别型数据。这种方式用于离散的类别数据，如分类标签、离散事件等。\n",
    "用途：适用于分类特征，例如细胞类型标签、基因突变状态等。\n",
    "实现：输入数据通常会进行独热编码（one-hot encoding）或嵌入编码（embedding encoding）处理\n",
    "\n",
    "Continuous (\"continuous\")\n",
    "解释：选择 \"continuous\" 作为嵌入风格时，输入数据被认为是连续型数据。这种方式通常用于数值数据或有序数据。\n",
    "用途：适用于数值型特征，例如基因表达数据、价格、温度等连续型变量。\n",
    "实现：输入数据直接以其原始数值形式输入模型，可能会进行标准化或归一化处理以提高模型性能。\n",
    "\n",
    "Scaling (\"scaling\")\n",
    "解释：选择 \"scaling\" 作为嵌入风格时，输入数据在输入模型之前会进行缩放处理。这种方式可以用于任意类型的数据，通过缩放使数据在统一的尺度范围内，通常在 [0, 1] 之间。\n",
    "用途：适用于需要统一尺度的特征，以防止某些特征在训练过程中由于数值范围过大或过小而影响模型性能。\n",
    "实现：输入数据通常会进行最小-最大缩放（min-max scaling）处理。\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "cell_emb_style: 定义如何从模型的输出中提取单个句子或文档的嵌入表示（embedding representation）\n",
    "\n",
    "Average Pooling (\"avg-pool\") 平均池化是对所有 token 的嵌入取平均值，以获取整个序列的表示。\n",
    "Weighted Pooling (\"w-pool\") 加权池化是对所有 token 的嵌入进行加权平均\n",
    "CLS Token Embedding (\"cls\") 用于分类任务，因为 [CLS] token 的嵌入专门设计用于捕获全局信息。\n",
    "\n",
    "'''\n",
    "\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False\n",
    "\n",
    "\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "\n",
    "input_emb_style = \"category\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "\n",
    "\n",
    "# adv_E_delay_epochs：编码器的对抗训练延迟几个 epoch 开始。\n",
    "# adv_D_delay_epochs：分类器的对抗训练延迟几个 epoch 开始。\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "\n",
    "\n",
    "mvc_decoder_style = \"inner product\"  # MVC 目标的解码器风格\n",
    "ecs_threshold = config.ecs_thres  # 控制 ECS 目标的相似性阈值\n",
    "dab_weight = config.dab_weight  # 控制 DAB 目标的权重,调整对抗训练在整体训练过程中的影响力。\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "print(\"explicit_zero_prob: \", explicit_zero_prob)\n",
    "\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "# 决定是否在每个序列批次进行采样\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"  选择不同的 Transformer 编码器（Encoder）实现方式: linear attention or flash attention\n",
    "\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 60  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6a644bb8-7525-45c3-8d19-2e6c4a4ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2811dbbd-9cf9-4807-a372-e7f176767a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_protein-Jul25-12-05\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7f708802-bb58-4975-a96b-8b1a61d981eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"./data/process_OLINK_data_nanToZero.h5ad\")\n",
    "# adata_test = sc.read(\"./data/protein_adata_withsex_notContainNaN_test.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0a5920f8-8922-4bb7-9e76-9aca2589da8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2920)\n"
     ]
    }
   ],
   "source": [
    "print(adata.shape)\n",
    "# print(adata_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "73d04d32-5b16-427a-bfa6-3fe0f4f67c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.neighbors(adata)\n",
    "# sc.tl.umap(adata)\n",
    "\n",
    "# sc.pp.neighbors(adata_test)\n",
    "# sc.tl.umap(adata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c2f4a29d-b8f2-4226-9272-488f7c6a9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"individual\"] = adata.obs.index\n",
    "# adata_test.obs[\"individual\"] = adata_test.obs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a37024b1-9a21-47d0-9832-5aed74b953fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将细胞类型信息转换为分类类型\n",
    "# 将细胞类型信息从字符串转换为分类数据类型 (category)\n",
    "# adata.obs[\"sextype\"] = adata.obs[\"sex\"].astype(\"category\")\n",
    "# adata_test.obs[\"sextype\"] = adata_test.obs[\"sex\"].astype(\"category\")\n",
    "\n",
    "# 设置批次信息，为了合并数据集后可以区分训练和测试数据集\n",
    "# adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "# adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"      \n",
    "\n",
    "# 设置基因名称为变量索引\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "# adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "\n",
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "\n",
    "# adata_test_raw = adata_test.copy() # 备份测试数据集\n",
    "\n",
    "# adata = adata.concatenate(adata_test, batch_key=\"str_batch\") # 合并训练和测试数据集，使用 `str_batch` 作为批次键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "270a92c8-69b9-4669-8692-4ff912beafcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2920"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_names = adata.var.index.tolist()\n",
    "len(gene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2231f321-4fdc-4709-802d-f05496c62e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并后\n",
    "\n",
    "# # make the batch category column\n",
    "# # 将批次标签转换为类别编码，并存储在 batch_id 列中\n",
    "# batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "# adata.obs[\"batch_id\"] = batch_id_labels\n",
    "\n",
    "# # 将细胞类型标签转换为类别编码，并存储在 celltype_id 列中\n",
    "# sextype_id_labels = adata.obs[\"sex\"].astype(\"category\").cat.codes.values\n",
    "# num_types = len(np.unique(sextype_id_labels))\n",
    "# sextypes = adata.obs[\"sex\"].unique()\n",
    "\n",
    "# adata.obs[\"sex_id\"] = sextype_id_labels # 将细胞类型 ID 添加到观察数据中\n",
    "\n",
    "# # 计算细胞类型的数量，并生成 ID 到类型的映射字典\n",
    "# id2type = dict(enumerate(adata.obs[\"sex\"].astype(\"category\").cat.categories))\n",
    "# print(len(id2type))\n",
    "\n",
    "\n",
    "#  # 更新基因名称为变量列\n",
    "# adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3294f0e5-3fcb-4a6d-b83e-370c2f16f54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age_Group\n",
       "50-55       8\n",
       "55-60     424\n",
       "60-65     726\n",
       "65-70     839\n",
       "70-75    1023\n",
       "75-80    1463\n",
       "80-85    1253\n",
       "85-90     264\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_group_counts = adata.obs['Age_Group'].value_counts().sort_index()\n",
    "age_group_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8c45140f-5adc-4155-90a2-67648b6057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['Age_Group_Code'] = adata.obs['Age_Group'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4546a71d-be35-4131-b3aa-f21de8cdc39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age_Group_Code\n",
       "5    1463\n",
       "6    1253\n",
       "4    1023\n",
       "3     839\n",
       "2     726\n",
       "1     424\n",
       "7     264\n",
       "0       8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_group_code_counts = adata.obs['Age_Group_Code'].value_counts()\n",
    "age_group_code_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bd572f3a-f66e-410d-ade0-3829c0342ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>DoB_year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>individual</th>\n",
       "      <th>Age_Group_Code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000221</th>\n",
       "      <td>0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>79</td>\n",
       "      <td>75-80</td>\n",
       "      <td>1000221</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000326</th>\n",
       "      <td>1</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>82</td>\n",
       "      <td>80-85</td>\n",
       "      <td>1000326</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000662</th>\n",
       "      <td>0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>68</td>\n",
       "      <td>65-70</td>\n",
       "      <td>1000662</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000823</th>\n",
       "      <td>0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>60</td>\n",
       "      <td>60-65</td>\n",
       "      <td>1000823</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000945</th>\n",
       "      <td>1</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>76</td>\n",
       "      <td>75-80</td>\n",
       "      <td>1000945</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733048</th>\n",
       "      <td>1</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>78</td>\n",
       "      <td>75-80</td>\n",
       "      <td>1733048</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733304</th>\n",
       "      <td>0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>79</td>\n",
       "      <td>75-80</td>\n",
       "      <td>1733304</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733421</th>\n",
       "      <td>1</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>62</td>\n",
       "      <td>60-65</td>\n",
       "      <td>1733421</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733844</th>\n",
       "      <td>1</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>81</td>\n",
       "      <td>80-85</td>\n",
       "      <td>1733844</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733986</th>\n",
       "      <td>0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>60</td>\n",
       "      <td>60-65</td>\n",
       "      <td>1733986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sex  DoB_year  Age Age_Group individual  Age_Group_Code\n",
       "Id                                                              \n",
       "1000221    0    1945.0   79     75-80    1000221               5\n",
       "1000326    1    1942.0   82     80-85    1000326               6\n",
       "1000662    0    1956.0   68     65-70    1000662               3\n",
       "1000823    0    1964.0   60     60-65    1000823               2\n",
       "1000945    1    1948.0   76     75-80    1000945               5\n",
       "...      ...       ...  ...       ...        ...             ...\n",
       "1733048    1    1946.0   78     75-80    1733048               5\n",
       "1733304    0    1945.0   79     75-80    1733304               5\n",
       "1733421    1    1962.0   62     60-65    1733421               2\n",
       "1733844    1    1943.0   81     80-85    1733844               6\n",
       "1733986    0    1964.0   60     60-65    1733986               2\n",
       "\n",
       "[6000 rows x 6 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a7d40965-7e7f-4269-893c-fea3fcf44c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agetype_id_labels = adata.obs[\"Age_Group\"].cat.codes.values\n",
    "# num_types = len(np.unique(agetype_id_labels))\n",
    "# print(num_types)\n",
    "\n",
    "# agetype = adata.obs[\"Age_Group\"].unique()\n",
    "\n",
    "# adata.obs[\"agetype_id\"] = agetype_id_labels # 将细胞类型 ID 添加到观察数据中\n",
    "\n",
    "# # 计算细胞类型的数量，并生成 ID 到类型的映射字典\n",
    "# id2type = dict(enumerate(adata.obs[\"Age_Group\"].astype(\"category\").cat.categories))\n",
    "# print(len(id2type))\n",
    "\n",
    "\n",
    " # 更新基因名称为变量列\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe5bac-5ed2-4be5-873c-2d0a83e6ce92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8c58ea51-cecc-4563-bff3-2321dc09a009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6 3 2 7 4 1 0]\n",
      "Total 8 of /the cluster labels.\n",
      "Age_Group_Code\n",
      "5    1463\n",
      "6    1253\n",
      "4    1023\n",
      "3     839\n",
      "2     726\n",
      "1     424\n",
      "7     264\n",
      "0       8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_clusters = adata.obs['Age_Group_Code'].unique()\n",
    "\n",
    "print(unique_clusters)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(f\"Total {num_clusters} of /the cluster labels.\")\n",
    "\n",
    "cluster_sizes = adata.obs['Age_Group_Code'].value_counts()\n",
    "print(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b370be0b-1488-4c6a-b67e-194ddb7c672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据中不包含负值\n"
     ]
    }
   ],
   "source": [
    "# 检查adata.X中的负值\n",
    "if np.any(adata.X < 0):\n",
    "    print(\"数据中包含负值\")\n",
    "else:\n",
    "    print(\"数据中不包含负值\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "afc83450-4158-485b-af5d-29e6bc2bc611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75756712, 1.05973776, 0.81593332, ..., 1.15026781, 0.        ,\n",
       "        0.        ],\n",
       "       [1.06278189, 2.29367793, 0.        , ..., 0.98589464, 0.73487288,\n",
       "        0.        ],\n",
       "       [1.04225113, 1.41731008, 0.94009768, ..., 0.96449787, 0.96967555,\n",
       "        1.18637255],\n",
       "       ...,\n",
       "       [0.92070645, 1.19198703, 1.19013236, ..., 1.19356213, 0.52167633,\n",
       "        0.8199579 ],\n",
       "       [0.87312188, 0.59920075, 0.78309161, ..., 0.84485812, 2.41546227,\n",
       "        1.1980134 ],\n",
       "       [1.08649695, 1.07747466, 0.96337697, ..., 0.90819564, 3.28254599,\n",
       "        0.86678221]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f4b6a-4d2a-4998-9555-ce5bc7bf528e",
   "metadata": {},
   "source": [
    "### Match the genes od the pre-train model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "faf288f2-fbd6-47ae-a826-0ae030b2c30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - adata match 2889/2920 genes in vocabulary of size 36574.\n",
      "scGPT - INFO - Resume model from ../pre_trained_model/scGPT_blood/best_model.pt, the model args will override the config ../pre_trained_model/scGPT_blood/args.json.\n",
      "embsize: 512\n",
      "nhead: 8\n",
      "d_hid: 512\n",
      "nlayers: 12\n",
      "n_layers_cls: 3\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = config.load_model\n",
    "    model_config_file = model_dir + \"/args.json\"\n",
    "    model_file = model_dir + \"/best_model.pt\"\n",
    "    vocab_file = model_dir + \"/vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    '''\n",
    "    标记和筛选在词汇表中的基因\n",
    "    # 在 adata.var 中添加一列 id_in_vocab，标记基因是否在词汇表中。如果基因在词汇表中，则值为 1，否则为 -1\n",
    "    '''\n",
    "    \n",
    "    # adata\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"adata match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # print(adata.shape)\n",
    "    gene_in_adata = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "\n",
    "    # 加载模型配置并覆盖默认配置\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "    print(\"embsize:\" ,embsize)\n",
    "    print(\"nhead:\" ,nhead)\n",
    "    print(\"d_hid:\" ,d_hid)\n",
    "    print(\"nlayers:\" ,nlayers)\n",
    "    print(\"n_layers_cls:\" ,n_layers_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c81dac4b-0615-4966-88ab-e2ec64d692a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View of AnnData object with n_obs × n_vars = 6000 × 2889\n",
      "    obs: 'sex', 'DoB_year', 'Age', 'Age_Group', 'individual', 'Age_Group_Code'\n",
      "    var: 'gene_name', 'id_in_vocab'\n"
     ]
    }
   ],
   "source": [
    "print(adata)\n",
    "# print(adata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "30204116-7b9c-4471-af49-7d24fb661e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - 111 ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "# 应用了数据预处理器，用于对 AnnData 对象进行一系列的预处理操作。这些操作包括基因过滤、细胞过滤、数据归一化、对数转换和数据分箱\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # 在 adata.layers 中使用的原始数据的键\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # 步骤1: 是否通过基因计数过滤\n",
    "    filter_cell_by_counts=False,  # 步骤2: 是否通过细胞计数过滤\n",
    "    \n",
    "    normalize_total=1,  # 步骤3: 是否归一化原始数据以及归一化的总和  一行相加的和\n",
    "    result_normed_key=\"X_normed\",  # 在 adata.layers 中存储归一化数据的键\n",
    "    \n",
    "    log1p=False,  # 步骤4: 是否对归一化数据进行 log1p 转换\n",
    "    result_log1p_key=\"X_log1p\",  # 存储 log1p 转换数据的键\n",
    "    \n",
    "    subset_hvg=False,  # 步骤5: 是否筛选高变基因\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",  # 高变基因筛选的风格\n",
    "    \n",
    "    binning=n_bins,  # 步骤6: 是否对原始数据进行分箱处理以及分箱数量\n",
    "    result_binned_key=\"X_binned\",  # 在 adata.layers 中存储分箱数据的键 !!!\n",
    ")\n",
    "\n",
    "# 分割训练集和测试集，通过str_batch\n",
    "# adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "# adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "\n",
    "# 应用预处理器\n",
    "preprocessor(adata, batch_key=None)\n",
    "# preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "71f8ac5b-2fa9-4991-958f-633b5f6cc3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 6000 × 2889\n",
      "    obs: 'sex', 'DoB_year', 'Age', 'Age_Group', 'individual', 'Age_Group_Code'\n",
      "    var: 'gene_name', 'id_in_vocab'\n",
      "    obsm: 'bin_edges'\n",
      "    layers: 'X_normed', 'X_binned'\n"
     ]
    }
   ],
   "source": [
    "print(adata)\n",
    "# print(adata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1597ef2d-eaa5-4586-a0ca-be97868a2df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75756712, 1.05973776, 0.81593332, ..., 1.15026781, 0.        ,\n",
       "        0.        ],\n",
       "       [1.06278189, 2.29367793, 0.        , ..., 0.98589464, 0.73487288,\n",
       "        0.        ],\n",
       "       [1.04225113, 1.41731008, 0.94009768, ..., 0.96449787, 0.96967555,\n",
       "        1.18637255],\n",
       "       ...,\n",
       "       [0.92070645, 1.19198703, 1.19013236, ..., 1.19356213, 0.52167633,\n",
       "        0.8199579 ],\n",
       "       [0.87312188, 0.59920075, 0.78309161, ..., 0.84485812, 2.41546227,\n",
       "        1.1980134 ],\n",
       "       [1.08649695, 1.07747466, 0.96337697, ..., 0.90819564, 3.28254599,\n",
       "        0.86678221]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a21fdc0c-b848-4ecd-b36d-cb44e5a931b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78896957, 1.1036657 , 0.84975515, ..., 1.19794837, 0.        ,\n",
       "        0.        ],\n",
       "       [0.96572812, 2.08421813, 0.        , ..., 0.89586225, 0.66776392,\n",
       "        0.        ],\n",
       "       [0.90196284, 1.2265384 , 0.81355938, ..., 0.83467527, 0.83915603,\n",
       "        1.02668535],\n",
       "       ...,\n",
       "       [0.91068314, 1.1790104 , 1.17717593, ..., 1.18056836, 0.51599707,\n",
       "        0.81103139],\n",
       "       [0.95236234, 0.65358142, 0.85416135, ..., 0.9215335 , 2.63467836,\n",
       "        1.30673951],\n",
       "       [1.12605316, 1.11670239, 0.99845074, ..., 0.94126041, 3.40205398,\n",
       "        0.89833924]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers['X_normed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8e46e991-3c3f-4551-b6a8-adb239a882fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 77, 20, ..., 88,  0,  0],\n",
       "       [51, 96,  0, ..., 34,  6,  0],\n",
       "       [35, 85, 17, ..., 22, 22, 60],\n",
       "       ...,\n",
       "       [28, 72, 72, ..., 72,  2, 14],\n",
       "       [36,  5, 20, ..., 30, 99, 88],\n",
       "       [69, 67, 42, ..., 31, 99, 23]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers['X_binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a5bad-00ac-4c99-8911-9eeeaf2d7169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2348451a-f228-4520-8fc8-b69532fed325",
   "metadata": {},
   "source": [
    "### 预处理后的数据按照一定比例分割为训练集和验证集¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ad1526a4-6b1c-4f3d-8b33-af20b50ab9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.index = adata.obs[\"individual\"]\n",
    "# adata_test.obs.index = adata_test.obs[\"individual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "79f0a54b-d143-4104-b96d-6b2e9183fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(6000, 2889)\n"
     ]
    }
   ],
   "source": [
    "# the values of this map coorespond to the keys in preprocessing\n",
    "# 通过提供的 input_style，从字典中获取相应的键\n",
    "input_layer_key = {  \n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",  # log(1+x)\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "\n",
    "# layers 是一个字典，可以存储多个数据矩阵，这些矩阵通常是在不同处理步骤中生成的。\n",
    "# 例如可以在预处理数据时生成归一化数据、对数转换数据或分箱数据，并将它们存储在 layers 中\n",
    "# 通过 adata.layers[input_layer_key]，可以访问 layers 字典中键为 input_layer_key 的数据层\n",
    "\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "\n",
    "# all_counts = X_binned\n",
    "print(np.allclose(adata.layers['X_binned'], all_counts, atol=1e-9))\n",
    "print(all_counts.shape)\n",
    "\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "# 提取细胞类型标签\n",
    "cellypes_labels = adata.obs[\"Age_Group_Code\"].tolist()  # make sure count from 0\n",
    "cellypes_labels = np.array(cellypes_labels)\n",
    "\n",
    "# # 提取批次标签\n",
    "# batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "# num_batch_types = len(set(batch_ids))\n",
    "# print(\"num_batch_types: \",num_batch_types)\n",
    "# batch_ids = np.array(batch_ids)\n",
    "\n",
    "# 分割数据集\n",
    "# 将all_counts、celltypes_labels 和 batch_ids 以相同的比例分为训练集和验证集\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    \n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    \n",
    "    # train_batch_labels,\n",
    "    # valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts,          # 输入数据\n",
    "    \n",
    "    cellypes_labels,    # 细胞类型标签\n",
    "    \n",
    "    # batch_ids,           # 批次标签\n",
    "    \n",
    "    test_size=0.1,       # 验证集大小设置为 10%\n",
    "    shuffle=True         # 在划分前随机打乱数据\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0684582d-48ed-4a5d-aa67-d84a8e0ce4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98  6 55 ... 79 99 39]\n",
      " [15  3 75 ... 62 54 82]\n",
      " [33 73  0 ... 63 58 78]\n",
      " ...\n",
      " [59 11 53 ... 72 20 96]\n",
      " [13  2 51 ... 52 71 87]\n",
      " [25 97  2 ... 43 75 39]]\n",
      "(5400, 2889)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "14ad118d-53f6-4a6a-a06d-5ba1e14c9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36514\n",
      "2889\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "print(gene_ids.max())\n",
    "print(len(gene_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0d6bf18e-929c-4c3e-9e97-bb22c0700d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36571\n",
      "36572\n",
      "36573\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"<pad>\"])\n",
    "print(vocab[\"<cls>\"])\n",
    "print(vocab[\"<eoc>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d7d29-6e3a-4116-add2-488669426c9c",
   "metadata": {},
   "source": [
    "### 对训练集进行标记化和填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "67851334-b322-4f62-8452-16508f2b1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 5400, \n",
      "\t feature length: 2890\n",
      "scGPT - INFO - valid set number of samples: 600, \n",
      "\t feature length: 2890\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. 将train_data 中的基因名称转换为标记\n",
    "2. 将标记化后的序列填充到固定长度 max_seq_len，不足的部分使用 pad_token 和 pad_value 填充\n",
    "3. 在每个序列的开头添加 <cls> 标记\n",
    "'''\n",
    "# pdb.set_trace()\n",
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len, # 填充后的序列长度不会超过原始数据中实际存在的最大长度，即取min(max_seq_len, max_actual_seq_len)\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "    \n",
    "# 对验证集进行标记化和填充\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5e789a6c-ffe2-4aaf-8d14-9d87d63f6d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[36572,  8381,  8385,  ...,  4939, 12884, 35421],\n",
      "        [36572,  8381,  8385,  ...,  4939, 12884, 35421],\n",
      "        [36572,  8381,  8385,  ...,  4939, 12884, 35421],\n",
      "        ...,\n",
      "        [36572,  8381,  8385,  ...,  4939, 12884, 35421],\n",
      "        [36572,  8381,  8385,  ...,  4939, 12884, 35421],\n",
      "        [36572,  8381,  8385,  ...,  4939, 12884, 35421]])\n",
      "tensor([[ 0., 98.,  6.,  ..., 79., 99., 39.],\n",
      "        [ 0., 15.,  3.,  ..., 62., 54., 82.],\n",
      "        [ 0., 33., 73.,  ..., 63., 58., 78.],\n",
      "        ...,\n",
      "        [ 0., 59., 11.,  ..., 72., 20., 96.],\n",
      "        [ 0., 13.,  2.,  ..., 52., 71., 87.],\n",
      "        [ 0., 25., 97.,  ..., 43., 75., 39.]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train['genes'])\n",
    "print(tokenized_train['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "81fc5c59-a676-4d7b-8c51-a8a3eb47fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    \n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    # tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    # tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    # if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "    #     train_sort_ids = np.argsort(train_batch_labels)\n",
    "    #     input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "    #     input_values_train = input_values_train[train_sort_ids]\n",
    "    #     target_values_train = target_values_train[train_sort_ids]\n",
    "    #     tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "    #     tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "\n",
    "    #     valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "    #     input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "    #     input_values_valid = input_values_valid[valid_sort_ids]\n",
    "    #     target_values_valid = target_values_valid[valid_sort_ids]\n",
    "    #     tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "    #     tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": tokenized_train[\"genes\"],\n",
    "        \"values\": masked_values_train,\n",
    "        # \"target_values\": tokenized_train[\"values\"],\n",
    "        # \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"celltype_labels\": tensor_celltype_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": tokenized_valid[\"genes\"],\n",
    "        \"values\": masked_values_valid,\n",
    "        # \"target_values\": tokenized_valid[\"values\"],\n",
    "        # \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"celltype_labels\": tensor_celltype_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e6e7b2eb-62b6-4152-bffb-0068c72e5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "# 用于处理序列数据，使其可以被 PyTorch 的数据加载器 DataLoader 使用\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    # 获取数据集长度\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    # 获取特定索引的数据å\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5a226a8c-a715-42cb-a619-f1bf018bab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    # Initial: per_seq_batch_sample = False\n",
    "    # if per_seq_batch_sample:\n",
    "    #     # find the indices of samples in each seq batch\n",
    "    #     subsets = []\n",
    "    #     batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "    #     print(batch_labels_array)\n",
    "    #     for batch_label in np.unique(batch_labels_array):\n",
    "    #         batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "    #         subsets.append(batch_indices)\n",
    "        \n",
    "    #     data_loader = DataLoader(\n",
    "    #         dataset=dataset,\n",
    "    #         batch_sampler=SubsetsBatchSampler(\n",
    "    #             subsets,\n",
    "    #             batch_size,\n",
    "    #             intra_subset_shuffle=intra_domain_shuffle,\n",
    "    #             inter_subset_shuffle=shuffle,\n",
    "    #             drop_last=drop_last,\n",
    "    #         ),\n",
    "    #         num_workers=num_workers,\n",
    "    #         pin_memory=True,\n",
    "    #     )\n",
    "    #     return data_loader\n",
    "\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53a3ec-b615-4a63-860d-2c351b54746b",
   "metadata": {},
   "source": [
    "### Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "869000f2-5ca1-4a5d-943a-29e5e26b4166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "freezing weights for: encoder.embedding.weight\n",
      "freezing weights for: encoder.enc_norm.weight\n",
      "freezing weights for: encoder.enc_norm.bias\n",
      "freezing weights for: value_encoder.embedding.weight\n",
      "freezing weights for: value_encoder.enc_norm.weight\n",
      "freezing weights for: value_encoder.enc_norm.bias\n",
      "scGPT - INFO - Total Pre freeze Params 5144457\n",
      "scGPT - INFO - Total Post freeze Params 449289\n",
      "\n",
      "\n",
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(36574, 128, padding_idx=36571)\n",
      "    (enc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): CategoryValueEncoder(\n",
      "    (embedding): Embedding(103, 128, padding_idx=101)\n",
      "    (enc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ExprDecoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cls_decoder): ClsDecoder(\n",
      "    (_decoder): ModuleList(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_layer): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      "  (sim): Similarity(\n",
      "    (cos): CosineSimilarity()\n",
      "  )\n",
      "  (creterion_cce): CrossEntropyLoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化模型\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=2, # 分类器的层数，设置为3\n",
    "    n_cls=num_types if CLS else 1, # 分类器的输出类别数量。如果 CLS 为 True，则使用 num_types，否则为 1\n",
    "    \n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    \n",
    "    pad_token=pad_token, # 填充标记，用于对齐不同长度的序列\n",
    "    pad_value=pad_value, # 填充值，用于对齐不同长度的序列。\n",
    "    \n",
    "    do_mvc=MVC, # 是否进行域自适应批归一化  // false\n",
    "    do_dab=DAB, # 是否使用批次标签 //false\n",
    "    use_batch_labels=INPUT_BATCH_LABELS, # false\n",
    "    num_batch_labels=num_batch_types,  # use_batch_labels =  false, 这里无作用\n",
    "    domain_spec_batchnorm=config.DSBN, # false\n",
    "    \n",
    "    input_emb_style=input_emb_style, # 输入嵌入的风格或类型 e.g, continuous\n",
    "    \n",
    "    n_input_bins=n_input_bins,\n",
    "    \n",
    "    cell_emb_style=cell_emb_style, # cls\n",
    "    mvc_decoder_style=mvc_decoder_style, # do_mvc = false, 这里无作用\n",
    "    \n",
    "    ecs_threshold=ecs_threshold,  # set to 0.0 false\n",
    "    explicit_zero_prob=explicit_zero_prob, # 是否显式地考虑零概率  // false\n",
    "    \n",
    "    # use_fast_transformer=fast_transformer, #是否使用快速 Transformer 实现  // True   config.fast_transformer\n",
    "    use_fast_transformer=False,\n",
    "    fast_transformer_backend=fast_transformer_backend, # 快速 Transformer 的后端实现  // flash attention\n",
    "    \n",
    "    # 前规范化：在每个子层（自注意力和前馈神经网络）之前进行规范化。这种方法可以让梯度更稳定。\n",
    "    # 后规范化：在每个子层之后进行规范化。这种方法是 Transformer 最初提出时使用的方式\n",
    "    pre_norm=config.pre_norm, # norm_scheme = \"pre\" if pre_norm else \"post\"  //pre_norm = false => post\n",
    ")\n",
    "\n",
    "# 加载预训练模型参数\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "# 计算冻结前需要更新的参数数量\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# 冻结部分模型参数\n",
    "for name, para in model.named_parameters():\n",
    "    # print(\"-\"*20)\n",
    "    # print(f\"name: {name}\")\n",
    "    \n",
    "    # initial config.freeze = True\n",
    "    # 如果参数名称包含 \"encoder\" 且不包含 \"transformer_encoder\"，则冻结该参数\n",
    "    if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name: \n",
    "    # if config.freeze and \"encoder\" in name:  \n",
    "    # if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "# 计算冻结后需要更新的参数数量\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(model)\n",
    "\n",
    "# 将模型移动到设备上并监视模型\n",
    "model.to(device)\n",
    "wandb.watch(model)\n",
    "\n",
    "# #  初始化对抗性判别器（如果需要）\n",
    "# if ADV:\n",
    "#     discriminator = AdversarialDiscriminator(\n",
    "#         d_model=embsize,\n",
    "#         n_cls=num_batch_types,\n",
    "#     ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# GeneEncoder：首先对输入的基因ID进行嵌入和标准化。\n",
    "# ContinuousValueEncoder：对连续值进行编码，包括线性变换、激活和标准化。\n",
    "# TransformerEncoder：应用多个Transformer编码器层，对前面的编码结果进行进一步处理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5a08dd84-013f-4b74-8197-4c3ba17da2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "\n",
    "# scaler: 使用自动混合精度的梯度缩放器 GradScaler，根据 config.amp（自动混合精度）是否启用来决定\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bc0a1d34-b9ec-4e22-a053-3d716e763648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # initial\n",
    "    # 训练或评估循环中的部分代码\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_cls = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        \n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        \n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        # target_values = batch_data[\"target_values\"].to(device)\n",
    "        \n",
    "        # batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            \n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                # batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            \n",
    "            # loss = 0.0\n",
    "            \n",
    "            if CLS:\n",
    "                # print(\"output_dict\\n: \",output_dict[\"cls_output\"].argmax(1))\n",
    "                # print(\"celltype_labels \\n :\",celltype_labels)\n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "                # loss = loss_cls\n",
    "                # metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                error_rate = 1 - (\n",
    "                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / celltype_labels.size(0)\n",
    "                \n",
    "        wandb.log({\"epoch\": epoch,\"train_loss\": loss_cls.item(), \"error_rate\": 1-error_rate})\n",
    "\n",
    "        # 反向传播与优化\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss_cls).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        \n",
    "        # 记录和日志\n",
    "        total_loss += loss_cls.item()\n",
    "        # total_cls += loss_cls.item()\n",
    "        total_error += error_rate\n",
    "            \n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            \n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_cls = total_cls / log_interval\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            \n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \"\n",
    "                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "            )\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_cls = 0\n",
    "            total_error = 0\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "# # 定义了wandb日志中的指标\n",
    "# def define_wandb_metrcis():\n",
    "#     wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f215e377-3bfa-40c3-b2ca-620634632ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型在验证数据集上的性能\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_num = 0.0\n",
    "    truth_num = 0\n",
    "\n",
    "    total = 0\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "\n",
    "            # print(\"input_gene_ids: \",len(input_gene_ids))\n",
    "            \n",
    "            # target_values = batch_data[\"target_values\"].to(device)\n",
    "            # batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            \n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "            # print(\"celltype_labels: \",len(celltype_labels))\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    # batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,\n",
    "                    CCE=CCE,\n",
    "                    MVC=MVC,\n",
    "                    ECS=ECS,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, celltype_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            \n",
    "            truth_num = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "\n",
    "            # print(\"valid_accuracy: \",accuracy / len(input_gene_ids))\n",
    "            \n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "            \n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "            \n",
    "            truth_num  += truth_num\n",
    "\n",
    "            # wandb.log({\"valid_loss\": loss, \"valid_total_loss\": total_loss, \"valid_accuracy\": accuracy})\n",
    "            \n",
    "    print(\"truth_num: \",truth_num)\n",
    "    print(\"total_num: \",total_num)\n",
    "    \n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num, total_num / len(input_gene_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c8876-dee0-4b32-a0f0-5840392412aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3fa224b-e18b-43fb-a2c0-8869a716b9cf",
   "metadata": {},
   "source": [
    "### Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "76abc358-f01e-4de1-b904-4eacd8ae5346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'auto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[247], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 13\u001b[0m train_data_pt, valid_data_pt \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msort_seq_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_seq_batch_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m prepare_dataloader(\n\u001b[1;32m     16\u001b[0m     train_data_pt,\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m prepare_dataloader(\n\u001b[1;32m     24\u001b[0m     valid_data_pt,\n\u001b[1;32m     25\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39meval_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m )\n",
      "Cell \u001b[0;32mIn[217], line 2\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(sort_seq_batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(sort_seq_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m----> 2\u001b[0m     masked_values_train \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_mask_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenized_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     masked_values_valid \u001b[38;5;241m=\u001b[39m random_mask_value(\n\u001b[1;32m      9\u001b[0m         tokenized_valid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m         mask_ratio\u001b[38;5;241m=\u001b[39mmask_ratio,\n\u001b[1;32m     11\u001b[0m         mask_value\u001b[38;5;241m=\u001b[39mmask_value,\n\u001b[1;32m     12\u001b[0m         pad_value\u001b[38;5;241m=\u001b[39mpad_value,\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom masking at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ratio of masked values in train: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(masked_values_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mmask_value)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(masked_values_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mpad_value)\u001b[38;5;241m.\u001b[39mcount_nonzero()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     )\n",
      "File \u001b[0;32m/data/mr423/scGPT/scgpt/tokenizer/gene_tokenizer.py:472\u001b[0m, in \u001b[0;36mrandom_mask_value\u001b[0;34m(values, mask_ratio, mask_value, pad_value)\u001b[0m\n\u001b[1;32m    470\u001b[0m     n_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(non_padding_idx) \u001b[38;5;241m*\u001b[39m mask_ratio)\n\u001b[1;32m    471\u001b[0m     mask_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(non_padding_idx, n_mask, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 472\u001b[0m     row[mask_idx] \u001b[38;5;241m=\u001b[39m mask_value\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(values)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'auto'"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "# define_wandb_metrcis()\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "for epoch in range(1, 5 + 1):\n",
    "\n",
    "    print(\"----------------------\")\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(model, loader=train_loader,)\n",
    "        \n",
    "    val_loss, val_err, accuracy = evaluate(model, loader=valid_loader,)\n",
    "\n",
    "    wandb.log({\"val_loss\": val_loss, \"val_err\": val_err, \"valid_accuracy\": accuracy})\n",
    "\n",
    "    # 记录和模型保存\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | err {val_err:5.4f} | accuracy {accuracy:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    # 学习率调度\n",
    "    scheduler.step()\n",
    "    # if DAB_separate_optim:\n",
    "    #     scheduler_dab.step()\n",
    "    # if ADV:\n",
    "    #     scheduler_D.step()\n",
    "    #     scheduler_E.step()\n",
    "\n",
    "torch.save(best_model.state_dict(), 'best_model.pth')\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2dfa18-4a47-4e05-9c51-c3c50dd6bb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "210a20ae-1393-444c-a623-19ab9dd4efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试数据上进行模型的推理，并计算模型的性能指标，如准确率、精确率、召回率和宏观F1分数\n",
    "\n",
    "# %% inference\n",
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    \n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    # label from the test data \n",
    "    celltypes_labels = adata.obs[\"sex_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "    # print(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    # print(batch_ids)\n",
    "\n",
    "    # 数据预处理\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    # 创建测试数据集和数据加载器\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        # \"target_values\": tokenized_test[\"values\"],\n",
    "        # \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "    }\n",
    "\n",
    "    # print(test_data_pt[\"celltype_labels\"])\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # 模型推理和性能评估\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = evaluate(model, loader=test_loader, return_raw=True,)\n",
    "    \n",
    "    \n",
    "    # 计算性能指标\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    # 记录和返回结果\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9957ad1-787c-4d8d-a0a9-a133057f4770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.4375\n",
      "valid_accuracy:  0.46875\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.625\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.6875\n",
      "valid_accuracy:  0.4375\n",
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.65625\n",
      "valid_accuracy:  0.375\n",
      "valid_accuracy:  0.625\n",
      "valid_accuracy:  0.4375\n",
      "valid_accuracy:  0.625\n",
      "valid_accuracy:  0.625\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.625\n",
      "valid_accuracy:  0.28125\n",
      "valid_accuracy:  0.75\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.34375\n",
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.40625\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.40625\n",
      "valid_accuracy:  0.625\n",
      "valid_accuracy:  0.46875\n",
      "valid_accuracy:  0.4375\n",
      "valid_accuracy:  0.4375\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.4375\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.46875\n",
      "valid_accuracy:  0.40625\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.46875\n",
      "valid_accuracy:  0.65625\n",
      "valid_accuracy:  0.46875\n",
      "valid_accuracy:  0.4375\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.5\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.40625\n",
      "valid_accuracy:  0.59375\n",
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.59375\n",
      "valid_accuracy:  0.625\n",
      "valid_accuracy:  0.5625\n",
      "valid_accuracy:  0.59375\n",
      "valid_accuracy:  0.375\n",
      "valid_accuracy:  0.53125\n",
      "valid_accuracy:  0.65625\n",
      "valid_accuracy:  0.59375\n",
      "valid_accuracy:  0.375\n",
      "scGPT - INFO - Accuracy: 0.520, Precision: 0.260, Recall: 0.500, Macro F1: 0.342\n",
      "         sex individual sextype batch_id str_batch  predictions\n",
      "Id                                                             \n",
      "1486103    1    1486103       1        1         1            0\n",
      "1486158    1    1486158       1        1         1            0\n",
      "1486214    0    1486214       0        1         1            0\n",
      "1486253    1    1486253       1        1         1            0\n",
      "1486372    1    1486372       1        1         1            0\n",
      "...      ...        ...     ...      ...       ...          ...\n",
      "1733048    1    1733048       1        1         1            0\n",
      "1733304    0    1733304       0        1         1            0\n",
      "1733421    1    1733421       1        1         1            0\n",
      "1733844    1    1733844       1        1         1            0\n",
      "1733986    0    1733986       0        1         1            0\n",
      "\n",
      "[2000 rows x 6 columns]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "adata_test_raw.obs[\"predictions\"] = [p for p in predictions]\n",
    "\n",
    "print(adata_test_raw.obs)\n",
    "\n",
    "# print(adata_test_raw.obs[\"predictions\"])\n",
    "\n",
    "unique_clusters = adata_test_raw.obs['predictions'].unique()\n",
    "\n",
    "print(unique_clusters)\n",
    "# num_clusters = len(unique_clusters)\n",
    "# print(f\"Total {num_clusters} of the cluster labels.\")\n",
    "\n",
    "# cluster_sizes = adata_test_raw.obs['predictions'].value_counts()\n",
    "# print(cluster_sizes)\n",
    "\n",
    "# # plot\n",
    "# palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "# palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "# palette_ = {c: palette_[i] for i, c in enumerate(sextypes)}\n",
    "\n",
    "# with plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n",
    "#     sc.pl.umap(\n",
    "#         adata_test_raw,\n",
    "#         color=[\"predictions\"],\n",
    "#         # color=[\"celltype\", \"predictions\"],\n",
    "#         palette=palette_,\n",
    "#         show=False,\n",
    "#     )\n",
    "#     plt.savefig(save_dir / \"results.png\", dpi=300)\n",
    "\n",
    "# save_dict = {\n",
    "#     \"predictions\": predictions,\n",
    "#     # \"labels\": labels,\n",
    "#     \"results\": results,\n",
    "#     \"id_maps\": id2type\n",
    "# }\n",
    "# with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(save_dict, f)\n",
    "\n",
    "# results[\"test/cell_umap\"] = wandb.Image(\n",
    "#     str(save_dir / \"results.png\"),\n",
    "#     caption=f\"predictions macro f1 {results['test/macro_f1']:.3f}\",\n",
    "# )\n",
    "\n",
    "# wandb.log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bbe8e-7d4d-4c56-a1d4-04c62b57c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f035e9c-f990-4ac4-83e7-3d7601829f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scgpt)",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
