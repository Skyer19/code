{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated temp directory: /data/mr423/tmp\n",
      "scgpt location:  /data/mr423/project/code/maskedValuePrediction/scgpt/__init__.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'scgpt' from '/data/mr423/project/code/maskedValuePrediction/scgpt/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 检查 tempfile 模块使用的临时文件目录\n",
    "temp_dir = tempfile.gettempdir()\n",
    "print(\"Updated temp directory:\", temp_dir)\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "new_path = '/data/mr423/project/maskedValuePrediction/'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "# relaod the scgpt files\n",
    "import scgpt\n",
    "print(\"scgpt location: \", scgpt.__file__)\n",
    "importlib.reload(scgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "\n",
    "import argparse\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"WANDB_MODE\"]= \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Settings for wandb mentior\n",
    "######################################################################\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    do_train=True,\n",
    "    load_model=\"/data/mr423/project/pre_trained_model/scGPT_human\",\n",
    "    n_bins=101,\n",
    "\n",
    "    epochs=1, # 50 !!!!!!!!!!!!  test only\n",
    "    lr=0.0001,\n",
    "    batch_size=32,   # 128 !!!!!!!!!!!!  test only\n",
    "\n",
    "    layer_size=128, # 128\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=8,  # number of heads in nn.MultiheadAttention\n",
    "    \n",
    "    dropout=0.0,  # dropout probability\n",
    "\n",
    "    use_fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    freeze = True, #freeze\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"masked-train\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
